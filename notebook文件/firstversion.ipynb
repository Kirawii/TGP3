{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8fea2b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T01:12:54.488433Z",
     "iopub.status.busy": "2025-06-11T01:12:54.487728Z",
     "iopub.status.idle": "2025-06-11T01:13:01.096890Z",
     "shell.execute_reply": "2025-06-11T01:13:01.096266Z"
    },
    "papermill": {
     "duration": 6.617375,
     "end_time": "2025-06-11T01:13:01.098337",
     "exception": false,
     "start_time": "2025-06-11T01:12:54.480962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from collections import Counter\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import entropy\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import datetime\n",
    "import time\n",
    "from itertools import product\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost.callback import EarlyStopping\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cbt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cbt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cbt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split # 需要导入 train_test_split\n",
    "import optuna\n",
    "from xgboost.callback import EarlyStopping as XgbEarlyStopping\n",
    "from lightgbm.callback import early_stopping as lgb_early_stopping\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cbt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import shap \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a6301f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T01:13:01.112155Z",
     "iopub.status.busy": "2025-06-11T01:13:01.111253Z",
     "iopub.status.idle": "2025-06-11T01:13:01.119914Z",
     "shell.execute_reply": "2025-06-11T01:13:01.119210Z"
    },
    "papermill": {
     "duration": 0.016599,
     "end_time": "2025-06-11T01:13:01.121150",
     "exception": false,
     "start_time": "2025-06-11T01:13:01.104551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\"\n",
    "    遍历 DataFrame 的所有列，修改数据类型以减少内存占用。\n",
    "    这个版本会特别跳过所有以 'n' 开头的列，让它们保持原始类型。\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    # 定义要跳过的列的特征（n系列）\n",
    "    n_series_prefix = 'n'\n",
    "    \n",
    "    for col in df.columns:\n",
    "        \n",
    "        # --- 关键修改：在这里加入判断条件 ---\n",
    "        # 如果列名以 'n' 开头（或者你可以用一个更精确的列表），则跳过这一列\n",
    "        if col.startswith(n_series_prefix):\n",
    "            continue # continue 会立即结束本次循环，进入下一列的处理\n",
    "            \n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        # 对非 object 和非 category 的数值列进行处理\n",
    "        if col_type != object and not pd.api.types.is_categorical_dtype(df[col]):\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            # 整型处理\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            \n",
    "            # 浮点型处理\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        \n",
    "\n",
    "\n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65cfccde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T01:13:01.134514Z",
     "iopub.status.busy": "2025-06-11T01:13:01.133933Z",
     "iopub.status.idle": "2025-06-11T01:13:09.043232Z",
     "shell.execute_reply": "2025-06-11T01:13:09.042242Z"
    },
    "papermill": {
     "duration": 7.91778,
     "end_time": "2025-06-11T01:13:09.045066",
     "exception": false,
     "start_time": "2025-06-11T01:13:01.127286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始训练集形状: (800000, 47)\n",
      "原始测试集形状: (200000, 47)\n",
      "合并后数据形状: (1000000, 47)\n",
      "Memory usage of dataframe is 617.79 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2509130/1381646419.py:22: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if col_type != object and not pd.api.types.is_categorical_dtype(df[col]):\n",
      "/tmp/ipykernel_2509130/1381646419.py:22: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if col_type != object and not pd.api.types.is_categorical_dtype(df[col]):\n",
      "/tmp/ipykernel_2509130/1381646419.py:22: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if col_type != object and not pd.api.types.is_categorical_dtype(df[col]):\n",
      "/tmp/ipykernel_2509130/1381646419.py:22: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if col_type != object and not pd.api.types.is_categorical_dtype(df[col]):\n",
      "/tmp/ipykernel_2509130/1381646419.py:22: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if col_type != object and not pd.api.types.is_categorical_dtype(df[col]):\n",
      "/tmp/ipykernel_2509130/1381646419.py:22: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if col_type != object and not pd.api.types.is_categorical_dtype(df[col]):\n",
      "/tmp/ipykernel_2509130/1381646419.py:22: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if col_type != object and not pd.api.types.is_categorical_dtype(df[col]):\n",
      "/tmp/ipykernel_2509130/1381646419.py:22: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if col_type != object and not pd.api.types.is_categorical_dtype(df[col]):\n",
      "/tmp/ipykernel_2509130/1381646419.py:22: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if col_type != object and not pd.api.types.is_categorical_dtype(df[col]):\n",
      "/tmp/ipykernel_2509130/1381646419.py:22: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if col_type != object and not pd.api.types.is_categorical_dtype(df[col]):\n",
      "/tmp/ipykernel_2509130/1381646419.py:22: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if col_type != object and not pd.api.types.is_categorical_dtype(df[col]):\n",
      "/tmp/ipykernel_2509130/1381646419.py:22: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if col_type != object and not pd.api.types.is_categorical_dtype(df[col]):\n",
      "/tmp/ipykernel_2509130/1381646419.py:22: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if col_type != object and not pd.api.types.is_categorical_dtype(df[col]):\n",
      "/tmp/ipykernel_2509130/1381646419.py:22: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if col_type != object and not pd.api.types.is_categorical_dtype(df[col]):\n",
      "/tmp/ipykernel_2509130/1381646419.py:22: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if col_type != object and not pd.api.types.is_categorical_dtype(df[col]):\n",
      "/tmp/ipykernel_2509130/1381646419.py:22: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if col_type != object and not pd.api.types.is_categorical_dtype(df[col]):\n",
      "/tmp/ipykernel_2509130/1381646419.py:22: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if col_type != object and not pd.api.types.is_categorical_dtype(df[col]):\n",
      "/tmp/ipykernel_2509130/1381646419.py:22: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if col_type != object and not pd.api.types.is_categorical_dtype(df[col]):\n",
      "/tmp/ipykernel_2509130/1381646419.py:22: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if col_type != object and not pd.api.types.is_categorical_dtype(df[col]):\n",
      "/tmp/ipykernel_2509130/1381646419.py:22: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if col_type != object and not pd.api.types.is_categorical_dtype(df[col]):\n",
      "/tmp/ipykernel_2509130/1381646419.py:22: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if col_type != object and not pd.api.types.is_categorical_dtype(df[col]):\n",
      "/tmp/ipykernel_2509130/1381646419.py:22: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if col_type != object and not pd.api.types.is_categorical_dtype(df[col]):\n",
      "/tmp/ipykernel_2509130/1381646419.py:22: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if col_type != object and not pd.api.types.is_categorical_dtype(df[col]):\n",
      "/tmp/ipykernel_2509130/1381646419.py:22: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if col_type != object and not pd.api.types.is_categorical_dtype(df[col]):\n",
      "/tmp/ipykernel_2509130/1381646419.py:22: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if col_type != object and not pd.api.types.is_categorical_dtype(df[col]):\n",
      "/tmp/ipykernel_2509130/1381646419.py:22: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if col_type != object and not pd.api.types.is_categorical_dtype(df[col]):\n",
      "/tmp/ipykernel_2509130/1381646419.py:22: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if col_type != object and not pd.api.types.is_categorical_dtype(df[col]):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 494.76 MB\n",
      "Decreased by 19.9%\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"./data/train.csv\")\n",
    "test_df = pd.read_csv(\"./data/testA.csv\")\n",
    "\n",
    "# 记录测试集的 ID，以便最后提交\n",
    "test_ids = test_df['id']\n",
    "\n",
    "# --- 合并数据 ---\n",
    "# 在合并前，给 test_df 添加一个 isDefault 列并填充 NaN，作为后续分离的依据\n",
    "test_df['isDefault'] = np.nan\n",
    "data = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "print(f\"原始训练集形状: {train_df.shape}\")\n",
    "print(f\"原始测试集形状: {test_df.shape}\")\n",
    "print(f\"合并后数据形状: {data.shape}\")\n",
    "data = reduce_mem_usage(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3da6aa2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T01:13:09.065016Z",
     "iopub.status.busy": "2025-06-11T01:13:09.064703Z",
     "iopub.status.idle": "2025-06-11T01:13:09.068783Z",
     "shell.execute_reply": "2025-06-11T01:13:09.067985Z"
    },
    "papermill": {
     "duration": 0.013269,
     "end_time": "2025-06-11T01:13:09.069969",
     "exception": false,
     "start_time": "2025-06-11T01:13:09.056700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82744bb9",
   "metadata": {
    "papermill": {
     "duration": 0.006122,
     "end_time": "2025-06-11T01:13:09.082397",
     "exception": false,
     "start_time": "2025-06-11T01:13:09.076275",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "先对数据进行缺失值处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64a7050d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T01:13:09.096115Z",
     "iopub.status.busy": "2025-06-11T01:13:09.095519Z",
     "iopub.status.idle": "2025-06-11T01:13:09.444368Z",
     "shell.execute_reply": "2025-06-11T01:13:09.443532Z"
    },
    "papermill": {
     "duration": 0.357158,
     "end_time": "2025-06-11T01:13:09.445686",
     "exception": false,
     "start_time": "2025-06-11T01:13:09.088528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 47 columns):\n",
      " #   Column              Non-Null Count    Dtype  \n",
      "---  ------              --------------    -----  \n",
      " 0   id                  1000000 non-null  int32  \n",
      " 1   loanAmnt            1000000 non-null  float32\n",
      " 2   term                1000000 non-null  int8   \n",
      " 3   interestRate        1000000 non-null  float32\n",
      " 4   installment         1000000 non-null  float32\n",
      " 5   grade               1000000 non-null  object \n",
      " 6   subGrade            1000000 non-null  object \n",
      " 7   employmentTitle     999999 non-null   float32\n",
      " 8   employmentLength    941459 non-null   object \n",
      " 9   homeOwnership       1000000 non-null  int8   \n",
      " 10  annualIncome        1000000 non-null  float32\n",
      " 11  verificationStatus  1000000 non-null  int8   \n",
      " 12  issueDate           1000000 non-null  object \n",
      " 13  isDefault           800000 non-null   float32\n",
      " 14  purpose             1000000 non-null  int8   \n",
      " 15  postCode            999999 non-null   float32\n",
      " 16  regionCode          1000000 non-null  int8   \n",
      " 17  dti                 999700 non-null   float32\n",
      " 18  delinquency_2years  1000000 non-null  float32\n",
      " 19  ficoRangeLow        1000000 non-null  float32\n",
      " 20  ficoRangeHigh       1000000 non-null  float32\n",
      " 21  openAcc             1000000 non-null  float32\n",
      " 22  pubRec              1000000 non-null  float32\n",
      " 23  pubRecBankruptcies  999479 non-null   float32\n",
      " 24  revolBal            1000000 non-null  float32\n",
      " 25  revolUtil           999342 non-null   float32\n",
      " 26  totalAcc            1000000 non-null  float32\n",
      " 27  initialListStatus   1000000 non-null  int8   \n",
      " 28  applicationType     1000000 non-null  int8   \n",
      " 29  earliesCreditLine   1000000 non-null  object \n",
      " 30  title               999999 non-null   float32\n",
      " 31  policyCode          1000000 non-null  float32\n",
      " 32  n0                  949619 non-null   float64\n",
      " 33  n1                  949619 non-null   float64\n",
      " 34  n2                  949619 non-null   float64\n",
      " 35  n3                  949619 non-null   float64\n",
      " 36  n4                  958367 non-null   float64\n",
      " 37  n5                  949619 non-null   float64\n",
      " 38  n6                  949619 non-null   float64\n",
      " 39  n7                  949619 non-null   float64\n",
      " 40  n8                  949618 non-null   float64\n",
      " 41  n9                  949619 non-null   float64\n",
      " 42  n10                 958367 non-null   float64\n",
      " 43  n11                 912673 non-null   float64\n",
      " 44  n12                 949619 non-null   float64\n",
      " 45  n13                 949619 non-null   float64\n",
      " 46  n14                 949619 non-null   float64\n",
      "dtypes: float32(19), float64(15), int32(1), int8(7), object(5)\n",
      "memory usage: 235.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data.info())\n",
    "# print(\"DataFrame 的形状 (行, 列):\", data.shape)\n",
    "# if 'employmentLength' in data.columns:\n",
    "#     print(\"'employmentLength' 列是否为空:\", data['employmentLength'].empty)\n",
    "# else:\n",
    "#     print(\"DataFrame 中不存在 'employmentLength' 列！\")\n",
    "\n",
    "# print(\"'employmentLength' 列中所有不同的数据有:\")\n",
    "# print(data['employmentLength'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9834488c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T01:13:09.472117Z",
     "iopub.status.busy": "2025-06-11T01:13:09.471819Z",
     "iopub.status.idle": "2025-06-11T01:28:00.276646Z",
     "shell.execute_reply": "2025-06-11T01:28:00.275837Z"
    },
    "papermill": {
     "duration": 890.813696,
     "end_time": "2025-06-11T01:28:00.277920",
     "exception": false,
     "start_time": "2025-06-11T01:13:09.464224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "缺失值情况及数据类型：\n",
      "                   缺失值数量     数据类型\n",
      "employmentTitle        1  float32\n",
      "employmentLength   58541   object\n",
      "isDefault         200000  float32\n",
      "postCode               1  float32\n",
      "title                  1  float32\n",
      "\n",
      "--- 开始对 n 系列特征进行行聚合统计 (使用 lambda) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregating n-features: 100%|██████████| 11/11 [09:50<00:00, 53.71s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n特征聚合处理后，数据形状: (1000000, 58)\n",
      "   credit_history_days  credit_history_months  earliesCreditLine_year  \\\n",
      "0                 8554                    281                    2001   \n",
      "1                 8281                    272                    2002   \n",
      "2                 6820                    224                    2006   \n",
      "3                 9377                    308                    1999   \n",
      "4                17320                    569                    1977   \n",
      "\n",
      "   earliesCreditLine_month  \n",
      "0                        8  \n",
      "1                        5  \n",
      "2                        5  \n",
      "3                        5  \n",
      "4                        8  \n",
      "缺失值情况及数据类型：\n",
      "                  缺失值数量     数据类型\n",
      "employmentTitle       1  float32\n",
      "isDefault        200000  float32\n",
      "postCode              1  float32\n",
      "title                 1  float32\n"
     ]
    }
   ],
   "source": [
    "#采用树模型，因此对数值列填充-999，丢失本身可能也是一种信息\n",
    "cols_to_fill = ['loanAmnt', 'interestRate', 'installment', 'annualIncome', 'dti', 'delinquency_2years', 'ficoRangeLow', 'ficoRangeHigh', 'openAcc', 'pubRec', 'pubRecBankruptcies', 'revolBal', 'revolUtil', 'totalAcc', 'n0', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14']\n",
    "data[cols_to_fill] = data[cols_to_fill].fillna(-999)\n",
    "# 获取有缺失值的列及其缺失值数量\n",
    "missing_values = data.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "\n",
    "# 获取对应列的数据类型\n",
    "missing_dtypes = data[missing_values.index].dtypes\n",
    "\n",
    "# 合并缺失值数量和数据类型\n",
    "missing_info = pd.DataFrame({\n",
    "    '缺失值数量': missing_values,\n",
    "    '数据类型': missing_dtypes\n",
    "})\n",
    "\n",
    "print('缺失值情况及数据类型：')\n",
    "print(missing_info)\n",
    "\n",
    "# n特征处理\n",
    "n_feat = ['n0', 'n1', 'n2', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', ]\n",
    "\n",
    "nameList = ['min', 'max', 'sum', 'mean', 'median', 'skew', 'std', \n",
    "            'mode', 'range', 'q25', 'q75']\n",
    "\n",
    "# statList 中使用 lambda 匿名函数替代自定义函数\n",
    "statList = [\n",
    "    'min', 'max', 'sum', 'mean', 'median', 'skew', 'std', \n",
    "    lambda x: x.mode().mean() if not x.mode().empty else np.nan, # 众数\n",
    "    lambda x: x.max() - x.min(),                                 # 值域\n",
    "    lambda x: x.quantile(0.25),                                  # 25分位数\n",
    "    lambda x: x.quantile(0.75)                                   # 75分位数\n",
    "]\n",
    "\n",
    "# --- 2. 执行聚合 ---\n",
    "print(\"\\n--- 开始对 n 系列特征进行行聚合统计 (使用 lambda) ---\")\n",
    "for i in tqdm(range(len(nameList)), desc=\"Aggregating n-features\"):\n",
    "    # 为了避免潜在的 SettingWithCopyWarning，使用 .loc 更安全\n",
    "    data.loc[:, f'n_feat_{nameList[i]}'] = data[n_feat].agg(statList[i], axis=1)\n",
    "\n",
    "print(f\"n特征聚合处理后，数据形状: {data.shape}\")\n",
    "\n",
    "\n",
    "# 对于等级特征，采用有序编码\n",
    "data['grade'] = data['grade'].map({'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7})\n",
    "data['subGrade'] = data['subGrade'].astype(str)\n",
    "grade_map = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7}\n",
    "data['subGrade'] = data['subGrade'].str[0].map(grade_map) * 5 + data['subGrade'].str[1].astype(int)\n",
    "\n",
    "#就业年限处理\n",
    "if pd.api.types.is_categorical_dtype(data['employmentLength']):\n",
    "    data['employmentLength'] = data['employmentLength'].astype('object')\n",
    "data['employmentLength'].fillna(-999, inplace=True)\n",
    "data.loc[data['employmentLength'] == '< 1 year', 'employmentLength'] = 0.5\n",
    "data.loc[data['employmentLength'] == '10+ years', 'employmentLength'] = 12\n",
    "mask = ~data['employmentLength'].isin([-999, 0.5, 12])\n",
    "data.loc[mask, 'employmentLength'] = data.loc[mask, 'employmentLength'].str.split(' ').str[0].astype(int)\n",
    "data['employmentLength'] = data['employmentLength'].astype(float)\n",
    "# unique_values = data['employmentLength'].unique()\n",
    "\n",
    "# print(\"'employmentLength' 列中所有不同的数据有:\")\n",
    "#贷款发放日期处理\n",
    "base_date = pd.to_datetime('2025-01-01')\n",
    "data['issueDate'] = pd.to_datetime(data['issueDate'])\n",
    "data = data.assign(\n",
    "    issueDate_year = data['issueDate'].dt.year,\n",
    "    issueDate_month = data['issueDate'].dt.month,\n",
    "    issueDate_day = (base_date - data['issueDate']).dt.days\n",
    ").assign(\n",
    "    issueDate_week = lambda df: (df['issueDate_day'] % 7 + 1).astype(int)\n",
    ")\n",
    "del data['issueDate']\n",
    "\n",
    "# \n",
    "data['earliesCreditLine_dt'] = pd.to_datetime(data['earliesCreditLine'], format='%b-%Y')\n",
    "data['credit_history_days'] = (base_date - data['earliesCreditLine_dt']).dt.days\n",
    "data['credit_history_months'] = (base_date.year - data['earliesCreditLine_dt'].dt.year) * 12 + \\\n",
    "                               (base_date.month - data['earliesCreditLine_dt'].dt.month)\n",
    "data['earliesCreditLine_year'] = data['earliesCreditLine_dt'].dt.year\n",
    "data['earliesCreditLine_month'] = data['earliesCreditLine_dt'].dt.month\n",
    "del data['earliesCreditLine_dt'], data['earliesCreditLine']\n",
    "\n",
    "print(data[['credit_history_days', 'credit_history_months', 'earliesCreditLine_year', 'earliesCreditLine_month']].head())\n",
    "\n",
    "\n",
    "# 获取有缺失值的列及其缺失值数量\n",
    "missing_values = data.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "\n",
    "# 获取对应列的数据类型\n",
    "missing_dtypes = data[missing_values.index].dtypes\n",
    "\n",
    "# 合并缺失值数量和数据类型\n",
    "missing_info = pd.DataFrame({\n",
    "    '缺失值数量': missing_values,\n",
    "    '数据类型': missing_dtypes\n",
    "})\n",
    "\n",
    "print('缺失值情况及数据类型：')\n",
    "print(missing_info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9210ffaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T01:28:00.293360Z",
     "iopub.status.busy": "2025-06-11T01:28:00.292684Z",
     "iopub.status.idle": "2025-06-11T01:28:00.524306Z",
     "shell.execute_reply": "2025-06-11T01:28:00.523489Z"
    },
    "papermill": {
     "duration": 0.240474,
     "end_time": "2025-06-11T01:28:00.525516",
     "exception": false,
     "start_time": "2025-06-11T01:28:00.285042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape (1000000, 91)\n"
     ]
    }
   ],
   "source": [
    "data['avg_income'] = data['annualIncome'] / data['employmentLength']\n",
    "data['total_income'] = data['annualIncome'] * data['employmentLength']\n",
    "data['avg_loanAmnt'] = data['loanAmnt'] / data['term']\n",
    "data['mean_interestRate'] = data['interestRate'] / data['term']\n",
    "data['all_installment'] = data['installment'] * data['term']\n",
    "\n",
    "data['rest_money_rate'] = data['avg_loanAmnt'] / (data['annualIncome'] + 0.1)  # 287个收入为0\n",
    "data['rest_money'] = data['annualIncome'] - data['avg_loanAmnt']\n",
    "\n",
    "data['closeAcc'] = data['totalAcc'] - data['openAcc']\n",
    "data['ficoRange_mean'] = (data['ficoRangeHigh'] + data['ficoRangeLow']) / 2\n",
    "\n",
    "data['rest_pubRec'] = data['pubRec'] - data['pubRecBankruptcies']\n",
    "\n",
    "data['rest_Revol'] = data['loanAmnt'] - data['revolBal']\n",
    "\n",
    "data['dis_time'] = data['issueDate_year'] - (2020 - data['earliesCreditLine_year'])\n",
    "for col in ['employmentTitle', 'grade', 'subGrade', 'regionCode', 'issueDate_month', 'postCode']:\n",
    "    data['{}_count'.format(col)] = data.groupby([col])['id'].transform('count')\n",
    "\n",
    "# 1. 月度还款占月收入比\n",
    "data['monthly_debt_to_income'] = data['installment'] / (data['annualIncome'] / 12 + 1e-6)\n",
    "\n",
    "# 2. 收入扣除分期后剩余 \n",
    "data['disposable_monthly_income'] = (data['annualIncome'] / 12) - data['installment']\n",
    "\n",
    "# 3. 贷款总额与年收入比 \n",
    "data['loan_to_income_ratio'] = data['loanAmnt'] / (data['annualIncome'] + 1e-6)\n",
    "\n",
    "# 1. 账户开立频率 (平均每个月开立多少个账户)\n",
    "\n",
    "data['acc_open_frequency'] = data['totalAcc'] / (data['credit_history_months'] + 1e-6)\n",
    "\n",
    "# 2. 平均每个账户的贷款额\n",
    "data['avg_loan_per_acc'] = data['loanAmnt'] / (data['totalAcc'] + 1e-6)\n",
    "\n",
    "# 1. 不良记录密度 (平均每个月有多少不良记录)\n",
    "data['pubRec_density'] = data['pubRec'] / (data['credit_history_months'] + 1e-6)\n",
    "\n",
    "# 2. 两年内逾期密度 (平均每个月有多少逾期)\n",
    "data['delinquency_density_2y'] = data['delinquency_2years'] / 24\n",
    "\n",
    "# 3. 循环信贷总额度估算\n",
    "\n",
    "data['estimated_revol_limit'] = data['revolBal'] / (data['revolUtil'] / 100 + 1e-6)\n",
    "\n",
    "# 4. 未使用的循环信贷额度\n",
    "data['unused_revol_credit'] = data['estimated_revol_limit'] - data['revolBal']\n",
    "\n",
    "\n",
    "\n",
    "print('data.shape', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "592d7cbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T01:28:00.541601Z",
     "iopub.status.busy": "2025-06-11T01:28:00.540829Z",
     "iopub.status.idle": "2025-06-11T01:28:00.852627Z",
     "shell.execute_reply": "2025-06-11T01:28:00.851888Z"
    },
    "papermill": {
     "duration": 0.32069,
     "end_time": "2025-06-11T01:28:00.853743",
     "exception": false,
     "start_time": "2025-06-11T01:28:00.533053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 开始进行三阶特征交叉 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Triple Crossing:  67%|██████▋   | 4/6 [00:00<00:00, 30.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已创建特征: grade_homeOwnership_term_count\n",
      "已创建特征: subGrade_homeOwnership_term_count\n",
      "已创建特征: regionCode_grade_term_count\n",
      "已创建特征: purpose_employmentLength_homeOwnership_count\n",
      "已创建特征: verificationStatus_purpose_homeOwnership_count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Triple Crossing: 100%|██████████| 6/6 [00:00<00:00, 29.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已创建特征: regionCode_postCode_grade_count\n",
      "\n",
      "--- 三阶特征交叉完成 ---\n",
      "处理后数据形状: (1000000, 97)\n",
      "\n",
      "新创建的三阶交叉特征预览:\n",
      "   grade_homeOwnership_term_count  \\\n",
      "0                            4241   \n",
      "1                           30888   \n",
      "2                           30888   \n",
      "3                           54686   \n",
      "4                           95706   \n",
      "\n",
      "   purpose_employmentLength_homeOwnership_count  \n",
      "0                                           109  \n",
      "1                                         16485  \n",
      "2                                         14078  \n",
      "3                                         18969  \n",
      "4                                           309  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"\\n--- 开始进行三阶特征交叉 ---\")\n",
    "\n",
    "triple_cross_combinations = [\n",
    "    # 核心信用等级 + 房屋状况 + 贷款期限\n",
    "    ['grade', 'homeOwnership', 'term'],\n",
    "    \n",
    "    # 子等级 + 房屋状况 + 贷款期限\n",
    "    ['subGrade', 'homeOwnership', 'term'],\n",
    "    \n",
    "    # 地理位置 + 等级 + 贷款期限\n",
    "    ['regionCode', 'grade', 'term'],\n",
    "    \n",
    "    # 贷款用途 + 工作年限 + 房屋状况\n",
    "    ['purpose', 'employmentLength', 'homeOwnership'],\n",
    "    \n",
    "    # 验证状态 + 贷款用途 + 房屋状况\n",
    "    ['verificationStatus', 'purpose', 'homeOwnership'],\n",
    "\n",
    "    # 两个地理位置特征 + 等级\n",
    "    ['regionCode', 'postCode', 'grade']\n",
    "]\n",
    "\n",
    "\n",
    "# --- 2. 循环创建三阶交叉特征 ---\n",
    "# 确保在运行这段代码前，这些列的缺失值已经被填充 (例如用 'MISSING' 或 -999)\n",
    "for f_list in tqdm(triple_cross_combinations, desc=\"Triple Crossing\"):\n",
    "    \n",
    "    # 创建新特征的列名，例如 'grade_homeOwnership_term_count'\n",
    "    feature_name = '_'.join(f_list) + '_count'\n",
    "    \n",
    "    # 使用 groupby 和 transform('count') 来计算共现次数\n",
    "    # transform 会返回一个与原始 DataFrame 索引相同的 Series，可以直接赋值\n",
    "    data[feature_name] = data.groupby(f_list)['id'].transform('count')\n",
    "    \n",
    "    print(f\"已创建特征: {feature_name}\")\n",
    "\n",
    "print(\"\\n--- 三阶特征交叉完成 ---\")\n",
    "print(f\"处理后数据形状: {data.shape}\")\n",
    "\n",
    "# (可选) 查看新创建的几个特征\n",
    "new_cols_to_preview = [\n",
    "    'grade_homeOwnership_term_count',\n",
    "    'purpose_employmentLength_homeOwnership_count'\n",
    "]\n",
    "# 检查列是否存在再打印\n",
    "cols_to_show = [col for col in new_cols_to_preview if col in data.columns]\n",
    "if cols_to_show:\n",
    "    print(\"\\n新创建的三阶交叉特征预览:\")\n",
    "    print(data[cols_to_show].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96cf2788",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T01:28:00.883471Z",
     "iopub.status.busy": "2025-06-11T01:28:00.883131Z",
     "iopub.status.idle": "2025-06-11T01:28:01.908859Z",
     "shell.execute_reply": "2025-06-11T01:28:01.907995Z"
    },
    "papermill": {
     "duration": 1.035074,
     "end_time": "2025-06-11T01:28:01.910224",
     "exception": false,
     "start_time": "2025-06-11T01:28:00.875150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 开始分离数据 ---\n",
      "分离完成: 训练集特征 (800000, 96), 测试集特征 (200000, 96)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 开始分离数据 ---\")\n",
    "\n",
    "# 训练集是 isDefault 不为空的行\n",
    "train_processed = data[data['isDefault'].notna()].copy()\n",
    "# 测试集是 isDefault 为空的行\n",
    "test_processed = data[data['isDefault'].isnull()].copy()\n",
    "\n",
    "# 分离出特征和目标\n",
    "X_all_train = train_processed.drop(columns=['isDefault'])\n",
    "y_all_train = train_processed['isDefault']\n",
    "y_all_train.name = 'isDefault'\n",
    "\n",
    "# 测试集特征\n",
    "X_test = test_processed.drop(columns=['isDefault'])\n",
    "\n",
    "print(f\"分离完成: 训练集特征 {X_all_train.shape}, 测试集特征 {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a002e0c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T01:28:01.928146Z",
     "iopub.status.busy": "2025-06-11T01:28:01.927337Z",
     "iopub.status.idle": "2025-06-11T01:28:31.055497Z",
     "shell.execute_reply": "2025-06-11T01:28:31.054705Z"
    },
    "papermill": {
     "duration": 29.138153,
     "end_time": "2025-06-11T01:28:31.056641",
     "exception": false,
     "start_time": "2025-06-11T01:28:01.918488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 开始构造精细化的群体统计特征 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping by Cat Feature: 100%|██████████| 6/6 [00:21<00:00,  3.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "处理新特征中可能产生的缺失值...\n",
      "--- 精细化群体统计特征构造完成 ---\n"
     ]
    }
   ],
   "source": [
    "def create_groupby_features(X_train, X_val):\n",
    "    \"\"\"\n",
    "    基于类别特征分组，为数值特征创建更精细的统计特征。(修正版)\n",
    "    \"\"\"\n",
    "    print(\"\\n--- 开始构造精细化的群体统计特征 ---\")\n",
    "    \n",
    "    cat_cols = ['grade', 'subGrade', 'purpose', 'homeOwnership', 'term', 'employmentLength']\n",
    "    num_cols = ['annualIncome', 'dti', 'interestRate', 'revolUtil', 'fico_mean', 'loanAmnt']\n",
    "    \n",
    "    if 'fico_mean' not in X_train.columns and 'ficoRangeHigh' in X_train.columns:\n",
    "        X_train['fico_mean'] = (X_train['ficoRangeHigh'] + X_train['ficoRangeLow']) / 2\n",
    "        X_val['fico_mean'] = (X_val['ficoRangeHigh'] + X_val['ficoRangeLow']) / 2\n",
    "\n",
    "    X_train_processed = X_train.copy()\n",
    "    X_val_processed = X_val.copy()\n",
    "    \n",
    "    for cat in tqdm(cat_cols, desc=\"Grouping by Cat Feature\"):\n",
    "        for num in num_cols:\n",
    "            \n",
    "            # --- 核心修正：改变 groupby 和 agg 的方式 ---\n",
    "            \n",
    "            # a. 定义命名聚合的字典\n",
    "            #    格式为：新列名 = (要聚合的旧列名, 聚合函数)\n",
    "            named_agg_dict = {\n",
    "                f'{cat}_{num}_mean': (num, 'mean'),\n",
    "                f'{cat}_{num}_std': (num, 'std'),\n",
    "                f'{cat}_{num}_max': (num, 'max'),\n",
    "                f'{cat}_{num}_min': (num, 'min'),\n",
    "                f'{cat}_{num}_median': (num, 'median'),\n",
    "            }\n",
    "            \n",
    "            # b. 在 DataFrameGroupBy 对象上 (X_train_processed.groupby(cat)) 执行命名聚合\n",
    "            #    注意，我们不再先选择 [num] 列\n",
    "            agg_map = X_train_processed.groupby(cat).agg(**named_agg_dict)\n",
    "            \n",
    "            # c. 将学习到的映射 merge 到训练集和验证集\n",
    "            X_train_processed = X_train_processed.merge(agg_map, on=cat, how='left')\n",
    "            X_val_processed = X_val_processed.merge(agg_map, on=cat, how='left')\n",
    "\n",
    "    # ... 后续的缺失值填充代码不变 ...\n",
    "    print(\"\\n处理新特征中可能产生的缺失值...\")\n",
    "    fill_value_map = {}\n",
    "    new_cols = [col for col in X_train_processed.columns if any(stat in col for stat in ['_mean', '_std', '_max', '_min', '_median'])]\n",
    "    \n",
    "    for col in new_cols:\n",
    "        if pd.api.types.is_numeric_dtype(X_train_processed[col]):\n",
    "            fill_value_map[col] = X_train_processed[col].median()\n",
    "\n",
    "    X_train_processed.fillna(value=fill_value_map, inplace=True)\n",
    "    X_val_processed.fillna(value=fill_value_map, inplace=True)\n",
    "    \n",
    "    print(\"--- 精细化群体统计特征构造完成 ---\")\n",
    "    return X_train_processed, X_val_processed\n",
    "\n",
    "X_all_train,X_test = create_groupby_features(X_all_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6af72c52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T01:28:31.074148Z",
     "iopub.status.busy": "2025-06-11T01:28:31.073594Z",
     "iopub.status.idle": "2025-06-11T01:29:03.325654Z",
     "shell.execute_reply": "2025-06-11T01:29:03.324910Z"
    },
    "papermill": {
     "duration": 32.261887,
     "end_time": "2025-06-11T01:29:03.326895",
     "exception": false,
     "start_time": "2025-06-11T01:28:31.065008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Categorical-Numerical Crossing: 100%|██████████| 9/9 [00:03<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "类别-数值特征交叉完成。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Categorical-Categorical Crossing: 100%|██████████| 36/36 [00:18<00:00,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "类别-类别特征交叉完成。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### 用数值特征对类别特征做统计刻画，挑跟isDefalut相关性最高的匿名特征\n",
    "cross_cat = ['subGrade', 'grade', 'employmentLength', 'term', 'homeOwnership', 'postCode', 'regionCode','employmentTitle','title']\n",
    "cross_num = ['dti', 'revolBal', 'revolUtil', 'ficoRangeHigh', 'interestRate', 'loanAmnt', 'installment', 'annualIncome', 'n14',\n",
    "             'n2', 'n3','n1', 'n9', 'n5', 'n7','n10']\n",
    "\n",
    "for cat in tqdm(cross_cat, desc=\"Categorical-Numerical Crossing\"):\n",
    "    for num in cross_num:\n",
    "        feature_name = f'{cat}_{num}_mean'\n",
    "        mean_map = X_all_train.groupby(cat)[num].mean()\n",
    "        X_all_train[feature_name] = X_all_train[cat].map(mean_map)\n",
    "        X_test[feature_name] = X_test[cat].map(mean_map)\n",
    "        global_num_mean = X_all_train[num].mean()\n",
    "        X_test[feature_name].fillna(global_num_mean, inplace=True)\n",
    "        X_all_train[feature_name].fillna(global_num_mean, inplace=True)\n",
    "print(\"类别-数值特征交叉完成。\") \n",
    "\n",
    "for cat1, cat2 in tqdm(list(combinations(cross_cat, 2)), desc=\"Categorical-Categorical Crossing\"):\n",
    "    feature_name = f'{cat1}_{cat2}_count'\n",
    "\n",
    "    count_map = X_all_train.groupby([cat1, cat2]).size() \n",
    "    count_map.name = feature_name # 给Series命名以便map\n",
    "    \n",
    "\n",
    "    train_tuples = list(zip(X_all_train[cat1], X_all_train[cat2]))\n",
    "    X_all_train[feature_name] = pd.Series(train_tuples).map(count_map)\n",
    "\n",
    "    val_tuples = list(zip(X_test[cat1], X_test[cat2]))\n",
    "    X_test[feature_name] = pd.Series(val_tuples).map(count_map)\n",
    "\n",
    "    X_all_train[feature_name].fillna(0, inplace=True) # 训练集内部也可能有组合未出现（如果map源于子集）\n",
    "    X_test[feature_name].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "print(\"类别-类别特征交叉完成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8067618",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T01:29:03.348922Z",
     "iopub.status.busy": "2025-06-11T01:29:03.348395Z",
     "iopub.status.idle": "2025-06-11T01:37:05.660941Z",
     "shell.execute_reply": "2025-06-11T01:37:05.660306Z"
    },
    "papermill": {
     "duration": 482.325129,
     "end_time": "2025-06-11T01:37:05.662430",
     "exception": false,
     "start_time": "2025-06-11T01:29:03.337301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [05:10<00:00, 28.25s/it]\n"
     ]
    }
   ],
   "source": [
    "for f_pair in tqdm([\n",
    "    ['subGrade', 'regionCode'], ['grade', 'regionCode'], ['subGrade', 'postCode'], ['grade', 'postCode'], ['employmentTitle','title'],\n",
    "    ['regionCode','title'], ['postCode','title'], ['homeOwnership','title'], ['homeOwnership','employmentTitle'],['homeOwnership','employmentLength'],\n",
    "    ['regionCode', 'postCode']\n",
    "]):\n",
    "    f1, f2 = f_pair[0], f_pair[1]\n",
    "\n",
    "    # --- 1. 共现次数 (Co-occurrence Count) ---\n",
    "    count_feature_name = f'{f1}_{f2}_count'\n",
    "    # a. 在训练集上学习组合计数的映射\n",
    "    count_map = X_all_train.groupby([f1, f2]).size()\n",
    "    # b. 应用到训练集和验证集\n",
    "    X_all_train[count_feature_name] = pd.MultiIndex.from_frame(X_all_train[[f1, f2]]).map(count_map).fillna(0)\n",
    "    X_test[count_feature_name] = pd.MultiIndex.from_frame(X_test[[f1, f2]]).map(count_map).fillna(0)\n",
    "\n",
    "    # --- 2. nunique 和 熵 ---\n",
    "    # a. f1 -> f2 (e.g., grade -> homeOwnership)\n",
    "    agg_map = X_all_train.groupby(f1)[f2].agg(['nunique', lambda x: entropy(x.value_counts(normalize=True))])\n",
    "    agg_map.columns = [f'{f1}_{f2}_nunique', f'{f1}_{f2}_ent']\n",
    "    X_all_train = X_all_train.merge(agg_map, on=f1, how='left')\n",
    "    X_test = X_test.merge(agg_map, on=f1, how='left')\n",
    "    \n",
    "    # b. f2 -> f1 (e.g., homeOwnership -> grade)\n",
    "    agg_map = X_all_train.groupby(f2)[f1].agg(['nunique', lambda x: entropy(x.value_counts(normalize=True))])\n",
    "    agg_map.columns = [f'{f2}_{f1}_nunique', f'{f2}_{f1}_ent']\n",
    "    X_all_train = X_all_train.merge(agg_map, on=f2, how='left')\n",
    "    X_test = X_test.merge(agg_map, on=f2, how='left')\n",
    "\n",
    "    # --- 3. 比例偏好 ---\n",
    "    # a. 计算单特征计数的映射\n",
    "    f1_count_map = X_all_train[f1].value_counts()\n",
    "    f2_count_map = X_all_train[f2].value_counts()\n",
    "    \n",
    "    # b. 应用到训练集和验证集，并计算比例\n",
    "    X_all_train[f1 + '_count'] = X_all_train[f1].map(f1_count_map)\n",
    "    X_all_train[f2 + '_count'] = X_all_train[f2].map(f2_count_map)\n",
    "    X_test[f1 + '_count'] = X_test[f1].map(f1_count_map).fillna(0)\n",
    "    X_test[f2 + '_count'] = X_test[f2].map(f2_count_map).fillna(0)\n",
    "    \n",
    "    prop_f1_in_f2_name = f'{f1}_in_{f2}_prop'\n",
    "    prop_f2_in_f1_name = f'{f2}_in_{f1}_prop'\n",
    "    \n",
    "    X_all_train[prop_f1_in_f2_name] = X_all_train[count_feature_name] / (X_all_train[f2 + '_count'] + 1e-6)\n",
    "    X_all_train[prop_f2_in_f1_name] = X_all_train[count_feature_name] / (X_all_train[f1 + '_count'] + 1e-6)\n",
    "    X_test[prop_f1_in_f2_name] = X_test[count_feature_name] / (X_test[f2 + '_count'] + 1e-6)\n",
    "    X_test[prop_f2_in_f1_name] = X_test[count_feature_name] / (X_test[f1 + '_count'] + 1e-6)\n",
    "\n",
    "    # c. 删除临时的单特征计数列\n",
    "    X_all_train.drop(columns=[f1 + '_count', f2 + '_count'], inplace=True)\n",
    "    X_test.drop(columns=[f1 + '_count', f2 + '_count'], inplace=True)\n",
    "\n",
    "# 必须在分割后进行\n",
    "for cat in ['grade', 'subGrade', 'purpose']:\n",
    "    for num in ['annualIncome', 'dti', 'interestRate']:\n",
    "        # 1. 在 X_all_train 上学习每个类别的均值\n",
    "        mean_map = X_all_train.groupby(cat)[num].mean()\n",
    "        \n",
    "        # 2. 将均值 merge 回 X_all_train 和 X_test\n",
    "        X_all_train = X_all_train.merge(mean_map.rename(f'{cat}_{num}_group_mean'), on=cat, how='left')\n",
    "        X_test = X_test.merge(mean_map.rename(f'{cat}_{num}_group_mean'), on=cat, how='left')\n",
    "        \n",
    "        # 3. 构造差异特征\n",
    "        X_all_train[f'{num}_minus_{cat}_mean'] = X_all_train[num] - X_all_train[f'{cat}_{num}_group_mean']\n",
    "        X_test[f'{num}_minus_{cat}_mean'] = X_test[num] - X_test[f'{cat}_{num}_group_mean']\n",
    "        \n",
    "        # 4. 构造比率特征\n",
    "        X_all_train[f'{num}_ratio_{cat}_mean'] = X_all_train[num] / (X_all_train[f'{cat}_{num}_group_mean'] + 1e-6)\n",
    "        X_test[f'{num}_ratio_{cat}_mean'] = X_test[num] / (X_test[f'{cat}_{num}_group_mean'] + 1e-6)\n",
    "\n",
    "        # 5. 清理临时的均值列\n",
    "        X_all_train.drop(columns=[f'{cat}_{num}_group_mean'], inplace=True)\n",
    "        X_test.drop(columns=[f'{cat}_{num}_group_mean'], inplace=True)\n",
    "        \n",
    "        # 6. 别忘了填充 X_test 中可能产生的 NaN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a790fca",
   "metadata": {
    "papermill": {
     "duration": 0.009803,
     "end_time": "2025-06-11T01:37:05.683231",
     "exception": false,
     "start_time": "2025-06-11T01:37:05.673428",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fa99259",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T01:37:05.704138Z",
     "iopub.status.busy": "2025-06-11T01:37:05.703890Z",
     "iopub.status.idle": "2025-06-11T01:38:01.335761Z",
     "shell.execute_reply": "2025-06-11T01:38:01.334909Z"
    },
    "papermill": {
     "duration": 55.643783,
     "end_time": "2025-06-11T01:38:01.336924",
     "exception": false,
     "start_time": "2025-06-11T01:37:05.693141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检查并填充目标编码列的缺失值...\n",
      "在 X_all_train 上执行 K-Fold 目标编码...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Training Set: 100%|██████████| 9/9 [01:03<00:00,  7.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "使用在 X_all_train 上学习的规则转换 X_test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transforming Validation Set: 100%|██████████| 9/9 [00:00<00:00, 14.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "目标编码完成！\n",
      "删除原始类别列...\n",
      "\n",
      "--- 最终结果预览 ---\n",
      "处理后的训练集形状: (800000, 513)\n",
      "处理后的验证集形状: (200000, 513)\n",
      "\n",
      "训练集中新创建的特征:\n",
      "   homeOwnership_target_mean  verificationStatus_target_mean  \\\n",
      "0                   0.207946                        0.236982   \n",
      "1                   0.171418                        0.238457   \n",
      "2                   0.171547                        0.236982   \n",
      "3                   0.231953                        0.209411   \n",
      "4                   0.231955                        0.238457   \n",
      "\n",
      "   purpose_target_mean  initialListStatus_target_mean  \\\n",
      "0             0.292164                       0.201685   \n",
      "1             0.211508                       0.196036   \n",
      "2             0.211328                       0.201685   \n",
      "3             0.169572                       0.195996   \n",
      "4             0.228326                       0.201779   \n",
      "\n",
      "   applicationType_target_mean  postCode_target_mean  regionCode_target_mean  \\\n",
      "0                     0.198194              0.194743                0.212162   \n",
      "1                     0.198409              0.152216                0.159421   \n",
      "2                     0.198194              0.212889                0.195704   \n",
      "3                     0.198519              0.244493                0.212885   \n",
      "4                     0.198409              0.208902                0.216808   \n",
      "\n",
      "   employmentTitle_target_mean  title_target_mean  \n",
      "0                     0.165919           0.301213  \n",
      "1                     0.191489           0.100000  \n",
      "2                     0.199512           0.217879  \n",
      "3                     1.000000           0.175186  \n",
      "4                     0.262069           0.240265  \n",
      "\n",
      "验证集中新创建的特征:\n",
      "   homeOwnership_target_mean  verificationStatus_target_mean  \\\n",
      "0                   0.171535                        0.147221   \n",
      "1                   0.171535                        0.147221   \n",
      "2                   0.232107                        0.237858   \n",
      "3                   0.171535                        0.209412   \n",
      "4                   0.232107                        0.209412   \n",
      "\n",
      "   purpose_target_mean  initialListStatus_target_mean  \\\n",
      "0             0.211370                       0.202008   \n",
      "1             0.175718                       0.202008   \n",
      "2             0.211370                       0.196024   \n",
      "3             0.169278                       0.202008   \n",
      "4             0.211370                       0.202008   \n",
      "\n",
      "   applicationType_target_mean  postCode_target_mean  regionCode_target_mean  \\\n",
      "0                     0.198493              0.219849                0.215436   \n",
      "1                     0.198493              0.197407                0.195115   \n",
      "2                     0.198493              0.260252                0.236190   \n",
      "3                     0.198493              0.207561                0.211695   \n",
      "4                     0.198493              0.189904                0.195115   \n",
      "\n",
      "   employmentTitle_target_mean  title_target_mean  \n",
      "0                     0.121951           0.217978  \n",
      "1                     0.199512           0.181684  \n",
      "2                     0.000000           0.217978  \n",
      "3                     0.129032           0.175291  \n",
      "4                     0.199512           0.217978  \n"
     ]
    }
   ],
   "source": [
    "target_encode_cols = [\n",
    "    'homeOwnership', 'verificationStatus', 'purpose', 'initialListStatus', \n",
    "    'applicationType', 'postCode', 'regionCode', 'employmentTitle', 'title'\n",
    "]\n",
    "\n",
    "print(\"检查并填充目标编码列的缺失值...\")\n",
    "for col in target_encode_cols:\n",
    "    if X_all_train[col].isnull().any():\n",
    "        X_all_train[col].fillna('MISSING', inplace=True)\n",
    "    if X_test[col].isnull().any():\n",
    "        X_test[col].fillna('MISSING', inplace=True)\n",
    "\n",
    "kfold_num = 5\n",
    "skf = KFold(n_splits=kfold_num, shuffle=True, random_state=42)\n",
    "global_mean = y_all_train.mean()\n",
    "\n",
    "print(\"在 X_all_train 上执行 K-Fold 目标编码...\")\n",
    "\n",
    "new_feature_names = []\n",
    "\n",
    "for f in tqdm(target_encode_cols, desc=\"Encoding Training Set\"):\n",
    "    \n",
    "    new_col_name = f'{f}_target_mean'\n",
    "    X_all_train[new_col_name] = 0\n",
    "    new_feature_names.append(new_col_name)\n",
    "\n",
    "    # K-Fold 交叉验证循环\n",
    "    for i, (trn_idx, val_idx) in enumerate(skf.split(X_all_train, y_all_train)):\n",
    "        # 将训练集再次分割成 K-1 折的 \"学习集\" 和 1 折的 \"应用集\"\n",
    "        trn_x, val_x = X_all_train.iloc[trn_idx], X_all_train.iloc[val_idx]\n",
    "        trn_y = y_all_train.iloc[trn_idx]\n",
    "        \n",
    "        # 在 K-1 折上计算每个类别的目标均值\n",
    "        target_mean_map = trn_y.groupby(trn_x[f]).mean()\n",
    "        \n",
    "        # 将计算出的均值应用到 1 折的 \"应用集\" 上\n",
    "        true_val_indices = val_x.index \n",
    "        X_all_train.loc[true_val_indices, new_col_name] = val_x[f].map(target_mean_map)\n",
    "\n",
    "    # 填充在 K-Fold 过程中可能产生的 NaN\n",
    "    X_all_train[new_col_name].fillna(global_mean, inplace=True)\n",
    "    \n",
    "\n",
    "print(\"\\n使用在 X_all_train 上学习的规则转换 X_test...\")\n",
    "\n",
    "for f in tqdm(target_encode_cols, desc=\"Transforming Validation Set\"):\n",
    "    new_col_name = f'{f}_target_mean'\n",
    "    \n",
    "    # 计算在整个 X_all_train 上的目标均值映射\n",
    "    overall_mean_map = y_all_train.groupby(X_all_train[f]).mean()\n",
    "    # 将这个规则应用到 X_test 上\n",
    "    X_test[new_col_name] = X_test[f].map(overall_mean_map)\n",
    "    # 填充 X_test 中可能产生的 NaN\n",
    "    X_test[new_col_name].fillna(global_mean, inplace=True)\n",
    "\n",
    "print(\"\\n目标编码完成！\")\n",
    "\n",
    "print(\"删除原始类别列...\")\n",
    "X_all_train.drop(columns=target_encode_cols, inplace=True)\n",
    "X_test.drop(columns=target_encode_cols, inplace=True)\n",
    "\n",
    "print(\"\\n--- 最终结果预览 ---\")\n",
    "print(\"处理后的训练集形状:\", X_all_train.shape)\n",
    "print(\"处理后的验证集形状:\", X_test.shape)\n",
    "print(\"\\n训练集中新创建的特征:\")\n",
    "print(X_all_train[new_feature_names].head())\n",
    "print(\"\\n验证集中新创建的特征:\")\n",
    "print(X_test[new_feature_names].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06486279",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T01:38:01.382480Z",
     "iopub.status.busy": "2025-06-11T01:38:01.382208Z",
     "iopub.status.idle": "2025-06-11T01:38:08.704402Z",
     "shell.execute_reply": "2025-06-11T01:38:08.703674Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 7.335276,
     "end_time": "2025-06-11T01:38:08.705583",
     "exception": false,
     "start_time": "2025-06-11T01:38:01.370307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning Fill Values: 100%|██████████| 513/513 [00:04<00:00, 104.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "应用填充规则到 X_all_train 和 X_test...\n",
      "\n",
      "--- 再次检查最终的缺失值情况 ---\n",
      "填充后，X_train 中总缺失值数量: 0\n",
      "填充后，X_val 中总缺失值数量: 0\n",
      "\n",
      "太棒了！所有缺失值都已成功处理完毕！\n"
     ]
    }
   ],
   "source": [
    "fill_value_map = {}\n",
    "for col in tqdm(X_all_train.columns, desc=\"Learning Fill Values\"):\n",
    "    # 我们只对数值类型的列计算中位数作为填充值\n",
    "    # 如果列是 object 类型（例如原始的类别列还没删），则不处理\n",
    "    if pd.api.types.is_numeric_dtype(X_all_train[col]):\n",
    "        fill_value_map[col] = X_all_train[col].median()\n",
    "\n",
    "# 3. 使用学习到的规则（fill_value_map）来统一填充训练集和验证集\n",
    "print(\"应用填充规则到 X_all_train 和 X_test...\")\n",
    "X_all_train.fillna(value=fill_value_map, inplace=True)\n",
    "X_test.fillna(value=fill_value_map, inplace=True)\n",
    "\n",
    "\n",
    "# --- 最终检查 ---\n",
    "print(\"\\n--- 再次检查最终的缺失值情况 ---\")\n",
    "train_missing_count = X_all_train.isnull().sum().sum()\n",
    "val_missing_count = X_test.isnull().sum().sum()\n",
    "\n",
    "print(f\"填充后，X_train 中总缺失值数量: {train_missing_count}\")\n",
    "print(f\"填充后，X_val 中总缺失值数量: {val_missing_count}\")\n",
    "\n",
    "if train_missing_count == 0 and val_missing_count == 0:\n",
    "    print(\"\\n太棒了！所有缺失值都已成功处理完毕！\")\n",
    "else:\n",
    "    print(\"\\n警告！仍然存在缺失值，请检查流程。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7958de3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T01:38:08.732494Z",
     "iopub.status.busy": "2025-06-11T01:38:08.731777Z",
     "iopub.status.idle": "2025-06-11T01:38:09.866585Z",
     "shell.execute_reply": "2025-06-11T01:38:09.865904Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 1.149148,
     "end_time": "2025-06-11T01:38:09.867649",
     "exception": false,
     "start_time": "2025-06-11T01:38:08.718501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 检查最终 X_all_train 的缺失值情况 ---\n",
      "X_all_train 中没有缺失值。\n",
      "\n",
      "--- 检查最终 X_test 的缺失值情况 ---\n",
      "X_test 中没有缺失值。\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 检查最终 X_all_train 的缺失值情况 ---\")\n",
    "missing_values_train = X_all_train.isnull().sum()\n",
    "missing_values_train = missing_values_train[missing_values_train > 0]\n",
    "\n",
    "if not missing_values_train.empty:\n",
    "    missing_dtypes_train = X_all_train[missing_values_train.index].dtypes\n",
    "    missing_info_train = pd.DataFrame({\n",
    "        '缺失值数量': missing_values_train,\n",
    "        '数据类型': missing_dtypes_train\n",
    "    })\n",
    "    print('X_all_train 中的缺失值情况及数据类型：')\n",
    "    print(missing_info_train)\n",
    "else:\n",
    "    print(\"X_all_train 中没有缺失值。\")\n",
    "\n",
    "\n",
    "# --- 检查 X_test 的缺失值情况 ---\n",
    "print(\"\\n--- 检查最终 X_test 的缺失值情况 ---\")\n",
    "missing_values_val = X_test.isnull().sum()\n",
    "missing_values_val = missing_values_val[missing_values_val > 0]\n",
    "\n",
    "if not missing_values_val.empty:\n",
    "    missing_dtypes_val = X_test[missing_values_val.index].dtypes\n",
    "    missing_info_val = pd.DataFrame({\n",
    "        '缺失值数量': missing_values_val,\n",
    "        '数据类型': missing_dtypes_val\n",
    "    })\n",
    "    print('X_test 中的缺失值情况及数据类型：')\n",
    "    print(missing_info_val)\n",
    "else:\n",
    "    print(\"X_test 中没有缺失值。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c81849c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T01:38:09.894384Z",
     "iopub.status.busy": "2025-06-11T01:38:09.893774Z",
     "iopub.status.idle": "2025-06-11T01:38:09.897797Z",
     "shell.execute_reply": "2025-06-11T01:38:09.897280Z"
    },
    "papermill": {
     "duration": 0.018289,
     "end_time": "2025-06-11T01:38:09.898929",
     "exception": false,
     "start_time": "2025-06-11T01:38:09.880640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集: (800000, 513), 验证集: (200000, 513)\n"
     ]
    }
   ],
   "source": [
    "print(f\"训练集: {X_all_train.shape}, 验证集: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df749204",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T01:38:10.027132Z",
     "iopub.status.busy": "2025-06-11T01:38:10.026840Z",
     "iopub.status.idle": "2025-06-11T02:04:01.202417Z",
     "shell.execute_reply": "2025-06-11T02:04:01.201536Z"
    },
    "papermill": {
     "duration": 1551.190733,
     "end_time": "2025-06-11T02:04:01.203688",
     "exception": false,
     "start_time": "2025-06-11T01:38:10.012955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 第一部分：通过交叉验证获取特征重要性 ---\n",
      "正在训练第 1/5 折...\n",
      "正在训练第 2/5 折...\n",
      "正在训练第 3/5 折...\n",
      "正在训练第 4/5 折...\n",
      "正在训练第 5/5 折...\n",
      "已生成 429 个特征的平均重要性排序。\n",
      "\n",
      "--- 第二部分：通过交叉验证测试不同数量的特征子集 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Feature Subsets:   7%|▋         | 1/15 [00:26<06:11, 26.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用 50 个特征, 平均 CV AUC: 0.72616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Feature Subsets:  13%|█▎        | 2/15 [01:04<07:11, 33.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用 80 个特征, 平均 CV AUC: 0.73632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Feature Subsets:  20%|██        | 3/15 [01:48<07:36, 38.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用 100 个特征, 平均 CV AUC: 0.73833\n"
     ]
    }
   ],
   "source": [
    "X_all_train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_all_train.columns]\n",
    "X_test.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_test.columns]\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 第一部分：通过交叉验证，获取稳健的特征重要性排序\n",
    "# ==============================================================================\n",
    "print(\"--- 第一部分：通过交叉验证获取特征重要性 ---\")\n",
    "\n",
    "# 1. 定义参数和 K-Fold\n",
    "params_for_sorting = {\n",
    "    'booster': 'gbtree', 'objective': 'binary:logistic', 'eval_metric': 'auc',\n",
    "    'min_child_weight': 5, 'max_depth': 8, 'subsample': 0.8, 'colsample_bytree': 0.6,\n",
    "    'eta': 0.05, 'seed': 42, 'nthread': -1, 'tree_method': 'gpu_hist'\n",
    "}\n",
    "k = 5\n",
    "folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# 2. 创建一个 DataFrame 来存储每一折的重要性\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "# 3. K-Fold 循环\n",
    "for i, (train_index, val_index) in enumerate(folds.split(X_all_train, y_all_train)):\n",
    "    print(f\"正在训练第 {i+1}/{k} 折...\")\n",
    "    # 分割数据\n",
    "    train_X, val_X = X_all_train.iloc[train_index], X_all_train.iloc[val_index]\n",
    "    train_y, val_y = y_all_train.iloc[train_index], y_all_train.iloc[val_index]\n",
    "    \n",
    "    # 创建 DMatrix\n",
    "    dtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "    dval = xgb.DMatrix(val_X, label=val_y)\n",
    "    watchlist = [(dtrain, 'train'), (dval, 'eval')]\n",
    "    \n",
    "    # 训练模型\n",
    "    model = xgb.train(\n",
    "        params_for_sorting, dtrain, num_boost_round=1000, \n",
    "        evals=watchlist, verbose_eval=False, early_stopping_rounds=100\n",
    "    )\n",
    "    \n",
    "    # 收集重要性\n",
    "    fold_importance = pd.Series(model.get_score(importance_type='gain'), name=f'fold_{i+1}')\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance], axis=1)\n",
    "\n",
    "# 4. 计算平均重要性并排序\n",
    "feature_importance_df.fillna(0, inplace=True)\n",
    "feature_importance_df['mean'] = feature_importance_df.mean(axis=1)\n",
    "sorted_features = feature_importance_df.sort_values(by='mean', ascending=False).index.tolist()\n",
    "\n",
    "print(f\"已生成 {len(sorted_features)} 个特征的平均重要性排序。\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 第二部分：通过交叉验证，循环测试不同的特征数量\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 第二部分：通过交叉验证测试不同数量的特征子集 ---\")\n",
    "\n",
    "n_features_to_test = [50, 80, 100, 120, 150, 200,230,250,280,300,320,340,360,380,400]\n",
    "results = {}\n",
    "params_for_testing = params_for_sorting.copy()\n",
    "\n",
    "for n in tqdm(n_features_to_test, desc=\"Testing Feature Subsets\"):\n",
    "    top_n_features = sorted_features[:n]\n",
    "    \n",
    "    fold_scores = []\n",
    "    # 同样使用 K-Fold 来评估，保证评估的可靠性\n",
    "    for i, (train_index, val_index) in enumerate(folds.split(X_all_train, y_all_train)):\n",
    "        train_X, val_X = X_all_train[top_n_features].iloc[train_index], X_all_train[top_n_features].iloc[val_index]\n",
    "        train_y, val_y = y_all_train.iloc[train_index], y_all_train.iloc[val_index]\n",
    "\n",
    "        dtrain_subset = xgb.DMatrix(train_X, label=train_y)\n",
    "        dval_subset = xgb.DMatrix(val_X, label=val_y)\n",
    "        watchlist_subset = [(dtrain_subset, 'train'), (dval_subset, 'eval')]\n",
    "        \n",
    "        model = xgb.train(params_for_testing, dtrain_subset, num_boost_round=1000,\n",
    "                          evals=watchlist_subset, verbose_eval=False, early_stopping_rounds=50)\n",
    "        \n",
    "        y_pred_proba = model.predict(dval_subset, iteration_range=(0, model.best_iteration))\n",
    "        fold_scores.append(roc_auc_score(val_y, y_pred_proba))\n",
    "        \n",
    "    results[n] = np.mean(fold_scores)\n",
    "    print(f\"使用 {n} 个特征, 平均 CV AUC: {results[n]:.5f}\")\n",
    "# ==============================================================================\n",
    "# 第三部分：分析并可视化结果 (这部分代码和之前一样)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n--- 第三部分：分析实验结果 ---\")\n",
    "\n",
    "results_series = pd.Series(results)\n",
    "print(\"不同特征数量下的 AUC 分数:\")\n",
    "print(results_series)\n",
    "\n",
    "best_n = results_series.idxmax()\n",
    "best_auc = results_series.max()\n",
    "print(f\"\\n最佳性能点: 使用 {best_n} 个特征时，AUC 达到最高值 {best_auc:.5f}\")\n",
    "\n",
    "performance_threshold = best_auc * 0.999\n",
    "cost_effective_n = results_series[results_series >= performance_threshold].index[0]\n",
    "print(f\"推荐的“性价比”点: 使用 {cost_effective_n} 个特征时，性能已达到最佳的99.9%以上。\")\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "results_series.plot(kind='line', marker='o', grid=True)\n",
    "plt.axvline(x=best_n, color='r', linestyle='--', label=f'Best AUC ({best_auc:.4f}) at {best_n} features')\n",
    "plt.axvline(x=cost_effective_n, color='g', linestyle='--', label=f'Cost-Effective Point at {cost_effective_n} features')\n",
    "plt.title('Model Performance vs. Number of Features (XGBoost Native API)')\n",
    "plt.xlabel('Number of Top Features Used')\n",
    "plt.ylabel('Validation AUC Score')\n",
    "plt.xticks(n_features_to_test)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# --- 最终决策 ---\n",
    "FINAL_FEATURE_COUNT = cost_effective_n\n",
    "final_features_list = sorted_features[:FINAL_FEATURE_COUNT]\n",
    "\n",
    "X_train_final = X_all_train[final_features_list]\n",
    "X_val_final = X_test[final_features_list]\n",
    "\n",
    "print(f\"\\n最终决定使用 {FINAL_FEATURE_COUNT} 个特征。\")\n",
    "print(f\"最终训练集形状: {X_train_final.shape}\")\n",
    "print(f\"最终验证集形状: {X_val_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e85b52c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T02:04:01.237029Z",
     "iopub.status.busy": "2025-06-11T02:04:01.236358Z",
     "iopub.status.idle": "2025-06-11T02:04:01.865244Z",
     "shell.execute_reply": "2025-06-11T02:04:01.864375Z"
    },
    "papermill": {
     "duration": 0.646632,
     "end_time": "2025-06-11T02:04:01.866730",
     "exception": false,
     "start_time": "2025-06-11T02:04:01.220098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FINAL_FEATURE_COUNT = 340\n",
    "final_features_list = sorted_features[:FINAL_FEATURE_COUNT]\n",
    "\n",
    "X_train_final = X_all_train[final_features_list]\n",
    "X_val_final = X_test[final_features_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa812fc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T02:04:01.899087Z",
     "iopub.status.busy": "2025-06-11T02:04:01.898792Z",
     "iopub.status.idle": "2025-06-11T02:04:01.903028Z",
     "shell.execute_reply": "2025-06-11T02:04:01.902364Z"
    },
    "papermill": {
     "duration": 0.021385,
     "end_time": "2025-06-11T02:04:01.904111",
     "exception": false,
     "start_time": "2025-06-11T02:04:01.882726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "最终决定使用 340 个特征。\n",
      "最终训练集形状: (800000, 340)\n",
      "最终验证集形状: (200000, 340)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n最终决定使用 {FINAL_FEATURE_COUNT} 个特征。\")\n",
    "print(f\"最终训练集形状: {X_train_final.shape}\")\n",
    "print(f\"最终验证集形状: {X_val_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b6a244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.9.25 environment at: /home/ljh/anaconda3/envs/d2l\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 5ms\u001b[0m\u001b[0m\n",
      "\u001b[2mUsing Python 3.9.25 environment at: /home/ljh/anaconda3/envs/d2l\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 5ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! uv pip install \"optuna-integration[xgboost]\"\n",
    "! uv pip install \"optuna-integration[lightgbm]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c17fd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      2\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mlgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m训练耗时:\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m秒\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/d2l/lib/python3.9/site-packages/lightgbm/engine.py:322\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    311\u001b[0m     cb(\n\u001b[1;32m    312\u001b[0m         callback\u001b[38;5;241m.\u001b[39mCallbackEnv(\n\u001b[1;32m    313\u001b[0m             model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    319\u001b[0m         )\n\u001b[1;32m    320\u001b[0m     )\n\u001b[0;32m--> 322\u001b[0m \u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/d2l/lib/python3.9/site-packages/lightgbm/basic.py:4155\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   4152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   4153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4154\u001b[0m _safe_call(\n\u001b[0;32m-> 4155\u001b[0m     \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4156\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4158\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4159\u001b[0m )\n\u001b[1;32m   4160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[1;32m   4161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "model = lgb.train(params, dtrain, num_boost_round=500)\n",
    "print(\"训练耗时:\", time.time() - start, \"秒\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d570474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'objective': 'binary', 'metric': 'auc', 'device': 'gpu', 'verbose': -1, 'num_iterations': 10}\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=50000, n_features=50)\n",
    "dtrain = lgb.Dataset(X, label=y)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"device\": \"gpu\",\n",
    "    \"verbose\": 1,          # ⚠ 显示日志\n",
    "}\n",
    "\n",
    "model = lgb.train(params, dtrain, num_boost_round=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2734407d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T02:04:06.294563Z",
     "iopub.status.busy": "2025-06-11T02:04:06.294252Z",
     "iopub.status.idle": "2025-06-11T03:31:38.100270Z",
     "shell.execute_reply": "2025-06-11T03:31:38.099426Z"
    },
    "papermill": {
     "duration": 5251.825855,
     "end_time": "2025-06-11T03:31:38.101474",
     "exception": false,
     "start_time": "2025-06-11T02:04:06.275619",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-09 10:30:26,352] A new study created in memory with name: no-name-0910b23f-af8d-40ca-91f4-2471047a20eb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 步骤 0: 降低数据精度 (float64 -> float32) ---\n",
      "数据精度已优化。\n",
      "\n",
      "--- 在全部数据上为 XGBOOST 搜索超参数 (内存优化版) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-09 10:30:49,559] Trial 0 finished with value: 0.7374920041573446 and parameters: {'learning_rate': 0.06548286551310704, 'max_depth': 10, 'subsample': 0.880496408753074, 'colsample_bytree': 0.7839647301580277}. Best is trial 0 with value: 0.7374920041573446.\n",
      "[I 2025-12-09 10:31:14,296] Trial 1 finished with value: 0.7361319987875982 and parameters: {'learning_rate': 0.022285945883097426, 'max_depth': 3, 'subsample': 0.9903084312304462, 'colsample_bytree': 0.6361515223345737}. Best is trial 0 with value: 0.7374920041573446.\n",
      "[I 2025-12-09 10:32:08,475] Trial 2 finished with value: 0.7431364108120322 and parameters: {'learning_rate': 0.01889062538409558, 'max_depth': 9, 'subsample': 0.5293121551246565, 'colsample_bytree': 0.7071828493946146}. Best is trial 2 with value: 0.7431364108120322.\n",
      "[I 2025-12-09 10:32:39,873] Trial 3 finished with value: 0.7410598140858847 and parameters: {'learning_rate': 0.04368789434707733, 'max_depth': 6, 'subsample': 0.8900652803890834, 'colsample_bytree': 0.5618787162270059}. Best is trial 2 with value: 0.7431364108120322.\n",
      "[I 2025-12-09 10:33:05,162] Trial 4 finished with value: 0.7423130904918079 and parameters: {'learning_rate': 0.06455565007161876, 'max_depth': 4, 'subsample': 0.6909741756732564, 'colsample_bytree': 0.8915794420791258}. Best is trial 2 with value: 0.7431364108120322.\n",
      "[I 2025-12-09 10:33:10,393] Trial 5 pruned. Trial was pruned at iteration 5.\n",
      "[I 2025-12-09 10:33:18,003] Trial 6 pruned. Trial was pruned at iteration 43.\n",
      "[I 2025-12-09 10:33:37,168] Trial 7 pruned. Trial was pruned at iteration 285.\n",
      "[I 2025-12-09 10:33:43,116] Trial 8 pruned. Trial was pruned at iteration 5.\n",
      "[I 2025-12-09 10:34:04,512] Trial 9 finished with value: 0.7437024557310192 and parameters: {'learning_rate': 0.08496856030625516, 'max_depth': 6, 'subsample': 0.7005355945886967, 'colsample_bytree': 0.5553595613443382}. Best is trial 9 with value: 0.7437024557310192.\n",
      "[I 2025-12-09 10:34:24,015] Trial 10 finished with value: 0.7410619728206845 and parameters: {'learning_rate': 0.09528417717296106, 'max_depth': 6, 'subsample': 0.7919711196462986, 'colsample_bytree': 0.5068747188875617}. Best is trial 9 with value: 0.7437024557310192.\n",
      "[I 2025-12-09 10:34:29,384] Trial 11 pruned. Trial was pruned at iteration 10.\n",
      "[I 2025-12-09 10:34:34,845] Trial 12 pruned. Trial was pruned at iteration 9.\n",
      "[I 2025-12-09 10:34:52,378] Trial 13 pruned. Trial was pruned at iteration 171.\n",
      "[I 2025-12-09 10:34:57,733] Trial 14 pruned. Trial was pruned at iteration 5.\n",
      "[I 2025-12-09 10:35:41,286] Trial 15 finished with value: 0.7454824448393925 and parameters: {'learning_rate': 0.02326392262249625, 'max_depth': 8, 'subsample': 0.5139948966735647, 'colsample_bytree': 0.6895987503672241}. Best is trial 15 with value: 0.7454824448393925.\n",
      "[I 2025-12-09 10:35:46,676] Trial 16 pruned. Trial was pruned at iteration 10.\n",
      "[I 2025-12-09 10:35:52,101] Trial 17 pruned. Trial was pruned at iteration 5.\n",
      "[I 2025-12-09 10:35:58,144] Trial 18 pruned. Trial was pruned at iteration 15.\n",
      "[I 2025-12-09 10:36:03,643] Trial 19 pruned. Trial was pruned at iteration 5.\n",
      "[I 2025-12-09 10:36:09,074] Trial 20 pruned. Trial was pruned at iteration 5.\n",
      "[I 2025-12-09 10:36:14,606] Trial 21 pruned. Trial was pruned at iteration 5.\n",
      "[I 2025-12-09 10:36:47,228] Trial 22 finished with value: 0.7445694780792388 and parameters: {'learning_rate': 0.02628642940637633, 'max_depth': 9, 'subsample': 0.5403525685440911, 'colsample_bytree': 0.755165212976299}. Best is trial 15 with value: 0.7454824448393925.\n",
      "[I 2025-12-09 10:36:53,231] Trial 23 pruned. Trial was pruned at iteration 16.\n",
      "[I 2025-12-09 10:37:30,425] Trial 24 finished with value: 0.743171690692498 and parameters: {'learning_rate': 0.03353641386346183, 'max_depth': 9, 'subsample': 0.6592489229289767, 'colsample_bytree': 0.8370423852641606}. Best is trial 15 with value: 0.7454824448393925.\n",
      "[I 2025-12-09 10:37:35,777] Trial 25 pruned. Trial was pruned at iteration 5.\n",
      "[I 2025-12-09 10:37:41,640] Trial 26 pruned. Trial was pruned at iteration 5.\n",
      "[I 2025-12-09 10:37:47,215] Trial 27 pruned. Trial was pruned at iteration 5.\n",
      "[I 2025-12-09 10:37:53,183] Trial 28 pruned. Trial was pruned at iteration 5.\n",
      "[I 2025-12-09 10:38:13,448] Trial 29 pruned. Trial was pruned at iteration 234.\n",
      "[I 2025-12-09 10:38:20,973] Trial 30 pruned. Trial was pruned at iteration 31.\n",
      "[I 2025-12-09 10:38:28,043] Trial 31 pruned. Trial was pruned at iteration 21.\n",
      "[I 2025-12-09 10:38:35,225] Trial 32 pruned. Trial was pruned at iteration 7.\n",
      "[I 2025-12-09 10:38:42,994] Trial 33 pruned. Trial was pruned at iteration 10.\n",
      "[I 2025-12-09 10:38:50,534] Trial 34 pruned. Trial was pruned at iteration 5.\n",
      "[I 2025-12-09 10:38:58,230] Trial 35 pruned. Trial was pruned at iteration 23.\n",
      "[I 2025-12-09 10:39:04,793] Trial 36 pruned. Trial was pruned at iteration 5.\n",
      "[I 2025-12-09 10:39:10,906] Trial 37 pruned. Trial was pruned at iteration 5.\n",
      "[I 2025-12-09 10:39:16,561] Trial 38 pruned. Trial was pruned at iteration 5.\n",
      "[I 2025-12-09 10:39:22,936] Trial 39 pruned. Trial was pruned at iteration 17.\n",
      "[I 2025-12-09 10:39:37,509] Trial 40 pruned. Trial was pruned at iteration 138.\n",
      "[I 2025-12-09 10:39:44,645] Trial 41 pruned. Trial was pruned at iteration 28.\n",
      "[I 2025-12-09 10:39:50,704] Trial 42 pruned. Trial was pruned at iteration 10.\n",
      "[I 2025-12-09 10:39:57,834] Trial 43 pruned. Trial was pruned at iteration 18.\n",
      "[I 2025-12-09 10:40:03,355] Trial 44 pruned. Trial was pruned at iteration 5.\n",
      "[I 2025-12-09 10:40:20,704] Trial 45 finished with value: 0.7445595155156919 and parameters: {'learning_rate': 0.06100498172496394, 'max_depth': 8, 'subsample': 0.5302498548229052, 'colsample_bytree': 0.6439029770515549}. Best is trial 15 with value: 0.7454824448393925.\n",
      "[I 2025-12-09 10:40:26,152] Trial 46 pruned. Trial was pruned at iteration 5.\n",
      "[I 2025-12-09 10:40:31,972] Trial 47 pruned. Trial was pruned at iteration 5.\n",
      "[I 2025-12-09 10:40:37,385] Trial 48 pruned. Trial was pruned at iteration 5.\n",
      "[I 2025-12-09 10:41:04,994] Trial 49 finished with value: 0.7428627837162215 and parameters: {'learning_rate': 0.06184205100569182, 'max_depth': 8, 'subsample': 0.7173875844581538, 'colsample_bytree': 0.5795900223473317}. Best is trial 15 with value: 0.7454824448393925.\n",
      "[I 2025-12-09 10:41:05,094] A new study created in memory with name: no-name-ed5ef2e0-8789-46b5-a7b6-b07531eb39f1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- XGBOOST 调优完成 ---\n",
      "最佳 Validation AUC: 0.745482\n",
      "最佳超参数组合:\n",
      "{'learning_rate': 0.02326392262249625, 'max_depth': 8, 'subsample': 0.5139948966735647, 'colsample_bytree': 0.6895987503672241}\n",
      "\n",
      "--- 在全部数据上为 LIGHTGBM 搜索超参数 (内存优化版) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to read file: /tmp/dep-a1099d.d\n",
      "Failed to read file: /tmp/dep-6fa31f.d\n",
      "Failed to read file: /tmp/dep-42342e.d\n",
      "1 warning generated.\n",
      "Failed to read file: /tmp/dep-53d65c.d\n",
      "1 warning generated.\n",
      "Failed to read file: /tmp/dep-b61156.d\n",
      "1 warning generated.\n",
      "Failed to read file: /tmp/dep-4f7480.d\n",
      "Failed to read file: /tmp/dep-2e0c11.d\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "Failed to read file: /tmp/dep-c63fa5.d\n",
      "1 warning generated.\n",
      "Failed to read file: /tmp/dep-ef9b5e.d\n",
      "Failed to read file: /tmp/dep-704819.d\n",
      "1 warning generated.\n",
      "Failed to read file: /tmp/dep-e595a1.d\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "Failed to read file: /tmp/dep-6c0681.d\n",
      "Failed to read file: /tmp/dep-747b41.d\n",
      "Failed to read file: /tmp/dep-0209d6.d\n",
      "Failed to read file: /tmp/dep-746cd8.d\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "Failed to read file: /tmp/dep-5bde17.d\n",
      "Failed to read file: /tmp/dep-0730dc.d\n",
      "Failed to read file: /tmp/dep-1409f4.d\n",
      "1 warning generated.\n",
      "Failed to read file: /tmp/dep-a06b93.d\n",
      "Failed to read file: /tmp/dep-1187d5.d\n",
      "1 warning generated.\n",
      "Failed to read file: /tmp/dep-f20d11.d\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "Failed to read file: /tmp/dep-418815.d\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "Failed to read file: /tmp/dep-42a4b9.d\n",
      "1 warning generated.\n",
      "Failed to read file: /tmp/dep-969f13.d\n",
      "Failed to read file: /tmp/dep-324b91.d\n",
      "Failed to read file: /tmp/dep-093064.d\n",
      "1 warning generated.\n",
      "Failed to read file: /tmp/dep-2a5a26.d\n",
      "1 warning generated.\n",
      "Failed to read file: /tmp/dep-069ab4.d\n",
      "1 warning generated.\n",
      "Failed to read file: /tmp/dep-34ad3b.d\n",
      "1 warning generated.\n",
      "Failed to read file: /tmp/dep-06d417.d\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "Failed to read file: /tmp/dep-510826.d\n",
      "Failed to read file: /tmp/dep-d402e3.d\n",
      "Failed to read file: /tmp/dep-9e9284.d\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "[I 2025-12-09 12:22:35,975] Trial 0 finished with value: 0.7397569493941268 and parameters: {'learning_rate': 0.012922101751637334, 'num_leaves': 78}. Best is trial 0 with value: 0.7397569493941268.\n",
      "[I 2025-12-09 15:20:48,841] Trial 1 finished with value: 0.7404976630098632 and parameters: {'learning_rate': 0.0278793029324036, 'num_leaves': 297}. Best is trial 1 with value: 0.7404976630098632.\n",
      "[I 2025-12-09 19:52:54,584] Trial 2 finished with value: 0.7422912277375802 and parameters: {'learning_rate': 0.011056651960918875, 'num_leaves': 213}. Best is trial 2 with value: 0.7422912277375802.\n",
      "[I 2025-12-09 21:10:23,887] Trial 3 finished with value: 0.7371008681206468 and parameters: {'learning_rate': 0.07781408388559179, 'num_leaves': 296}. Best is trial 2 with value: 0.7422912277375802.\n",
      "[W 2025-12-09 21:45:04,355] Trial 4 failed with parameters: {'learning_rate': 0.06710375544177007, 'num_leaves': 300} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ljh/anaconda3/envs/d2l/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 205, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_4062009/4218649431.py\", line 94, in <lambda>\n",
      "    study.optimize(lambda trial: objective_func(trial, X_all_train, y_all_train), n_trials=50, gc_after_trial=True)\n",
      "  File \"/tmp/ipykernel_4062009/4218649431.py\", line 55, in objective_lgb_final_mem_opt\n",
      "    model = lgb.train(params, dtrain, num_boost_round=1000, valid_sets=[dval],\n",
      "  File \"/home/ljh/anaconda3/envs/d2l/lib/python3.9/site-packages/lightgbm/engine.py\", line 322, in train\n",
      "    booster.update(fobj=fobj)\n",
      "  File \"/home/ljh/anaconda3/envs/d2l/lib/python3.9/site-packages/lightgbm/basic.py\", line 4155, in update\n",
      "    _LIB.LGBM_BoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n",
      "[W 2025-12-09 21:45:04,358] Trial 4 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 94\u001b[0m\n\u001b[1;32m     91\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m, pruner\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39mpruners\u001b[38;5;241m.\u001b[39mMedianPruner(n_warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# 使用 lambda 将 X_all_train 和 y_all_train 传入\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_all_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_all_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     best_params_all_final[model_name] \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams\n\u001b[1;32m     96\u001b[0m     best_scores_all_final[model_name] \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/anaconda3/envs/d2l/lib/python3.9/site-packages/optuna/study/study.py:490\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    390\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    398\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    399\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 490\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/d2l/lib/python3.9/site-packages/optuna/study/_optimize.py:67\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 67\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/d2l/lib/python3.9/site-packages/optuna/study/_optimize.py:164\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     frozen_trial_id \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/anaconda3/envs/d2l/lib/python3.9/site-packages/optuna/study/_optimize.py:262\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    258\u001b[0m     updated_state \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    261\u001b[0m ):\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trial\u001b[38;5;241m.\u001b[39m_trial_id\n",
      "File \u001b[0;32m~/anaconda3/envs/d2l/lib/python3.9/site-packages/optuna/study/_optimize.py:205\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    208\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[33], line 94\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     91\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m, pruner\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39mpruners\u001b[38;5;241m.\u001b[39mMedianPruner(n_warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# 使用 lambda 将 X_all_train 和 y_all_train 传入\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43mobjective_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_all_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_all_train\u001b[49m\u001b[43m)\u001b[49m, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, gc_after_trial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     95\u001b[0m     best_params_all_final[model_name] \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams\n\u001b[1;32m     96\u001b[0m     best_scores_all_final[model_name] \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mvalue\n",
      "Cell \u001b[0;32mIn[33], line 55\u001b[0m, in \u001b[0;36mobjective_lgb_final_mem_opt\u001b[0;34m(trial, X_all, y_all)\u001b[0m\n\u001b[1;32m     53\u001b[0m dval \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mDataset(X_v, label\u001b[38;5;241m=\u001b[39my_v)\n\u001b[1;32m     54\u001b[0m pruning_callback \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mintegration\u001b[38;5;241m.\u001b[39mLightGBMPruningCallback(trial, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid_0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mlgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mdval\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mpruning_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m auc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbest_score[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid_0\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model, dtrain, dval, X_t, X_v, y_t, y_v; gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[0;32m~/anaconda3/envs/d2l/lib/python3.9/site-packages/lightgbm/engine.py:322\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    311\u001b[0m     cb(\n\u001b[1;32m    312\u001b[0m         callback\u001b[38;5;241m.\u001b[39mCallbackEnv(\n\u001b[1;32m    313\u001b[0m             model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    319\u001b[0m         )\n\u001b[1;32m    320\u001b[0m     )\n\u001b[0;32m--> 322\u001b[0m \u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/d2l/lib/python3.9/site-packages/lightgbm/basic.py:4155\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   4152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   4153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4154\u001b[0m _safe_call(\n\u001b[0;32m-> 4155\u001b[0m     \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4156\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4158\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4159\u001b[0m )\n\u001b[1;32m   4160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[1;32m   4161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# 0. 准备工作：降低数据精度\n",
    "# ==============================================================================\n",
    "# 假设此时你已经有了处理好的 X_all_train, y_all_train\n",
    "print(\"--- 步骤 0: 降低数据精度 (float64 -> float32) ---\")\n",
    "\n",
    "def downcast_floats(df):\n",
    "    float_cols = df.select_dtypes(include=['float64']).columns\n",
    "    for col in float_cols:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    return df\n",
    "\n",
    "X_all_train = downcast_floats(X_all_train)\n",
    "print(\"数据精度已优化。\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. 修改目标函数：内部进行临时分割和垃圾回收\n",
    "# ==============================================================================\n",
    "\n",
    "def objective_xgb_final_mem_opt(trial, X_all, y_all):\n",
    "    \"\"\"XGBoost 的目标函数 (最终预测阶段的内存优化版)\"\"\"\n",
    "    X_t, X_v, y_t, y_v = train_test_split(X_all, y_all, test_size=0.1, random_state=trial.number, stratify=y_all)\n",
    "    params = {\n",
    "        'objective': 'binary:logistic', 'eval_metric': 'auc', 'booster': 'gbtree',\n",
    "        'tree_method': 'gpu_hist', 'random_state': 42,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "    }\n",
    "    dtrain = xgb.DMatrix(X_t, label=y_t)\n",
    "    dval = xgb.DMatrix(X_v, label=y_v)\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, 'validation-auc')\n",
    "    model = xgb.train(params, dtrain, num_boost_round=1000,\n",
    "                      evals=[(dval, 'validation')],\n",
    "                      callbacks=[pruning_callback, xgb.callback.EarlyStopping(100)],\n",
    "                      verbose_eval=False)\n",
    "    auc = model.best_score\n",
    "    del model, dtrain, dval, X_t, X_v, y_t, y_v; gc.collect()\n",
    "    return auc\n",
    "\n",
    "def objective_lgb_final_mem_opt(trial, X_all, y_all):\n",
    "    \"\"\"LightGBM 的目标函数 (最终预测阶段的内存优化版)\"\"\"\n",
    "    X_t, X_v, y_t, y_v = train_test_split(X_all, y_all, test_size=0.1, random_state=trial.number, stratify=y_all)\n",
    "    params = {\n",
    "        'objective': 'binary', 'metric': 'auc', 'boosting_type': 'gbdt',\n",
    "        'device': 'gpu', 'random_state': 42, 'n_jobs': -1, 'verbose': -1,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "    }\n",
    "    dtrain = lgb.Dataset(X_t, label=y_t)\n",
    "    dval = lgb.Dataset(X_v, label=y_v)\n",
    "    pruning_callback = optuna.integration.LightGBMPruningCallback(trial, 'auc', 'valid_0')\n",
    "    model = lgb.train(params, dtrain, num_boost_round=1000, valid_sets=[dval],\n",
    "                      callbacks=[pruning_callback, lgb.early_stopping(100, verbose=False)])\n",
    "    auc = model.best_score['valid_0']['auc']\n",
    "    del model, dtrain, dval, X_t, X_v, y_t, y_v; gc.collect()\n",
    "    return auc\n",
    "\n",
    "def objective_cat_final_mem_opt(trial, X_all, y_all):\n",
    "    \"\"\"CatBoost 的目标函数 (最终预测阶段的内存优化版)\"\"\"\n",
    "    X_t, X_v, y_t, y_v = train_test_split(X_all, y_all, test_size=0.1, random_state=trial.number, stratify=y_all)\n",
    "    params = {\n",
    "        'objective': 'Logloss', 'eval_metric': 'AUC', 'task_type': 'GPU',\n",
    "        'random_seed': 42, 'verbose': 0,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'iterations': 1000, 'early_stopping_rounds': 100\n",
    "    }\n",
    "    model = cbt.CatBoostClassifier(**params)\n",
    "    model.fit(X_t, y_t, eval_set=[(X_v, y_v)], use_best_model=True)\n",
    "    auc = model.get_best_score()['validation']['AUC']\n",
    "    del model, X_t, X_v, y_t, y_v; gc.collect()\n",
    "    return auc\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. 运行统一的调优框架\n",
    "# ==============================================================================\n",
    "models_to_tune_final = {\n",
    "    'xgboost': objective_xgb_final_mem_opt,\n",
    "    'lightgbm': objective_lgb_final_mem_opt,\n",
    "    'catboost': objective_cat_final_mem_opt\n",
    "}\n",
    "\n",
    "best_params_all_final = {}\n",
    "best_scores_all_final = {}\n",
    "\n",
    "for model_name, objective_func in models_to_tune_final.items():\n",
    "    print(f\"\\n--- 在全部数据上为 {model_name.upper()} 搜索超参数 (内存优化版) ---\")\n",
    "    study = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
    "    try:\n",
    "        # 使用 lambda 将 X_all_train 和 y_all_train 传入\n",
    "        study.optimize(lambda trial: objective_func(trial, X_all_train, y_all_train), n_trials=50, gc_after_trial=True)\n",
    "        best_params_all_final[model_name] = study.best_trial.params\n",
    "        best_scores_all_final[model_name] = study.best_trial.value\n",
    "        print(f\"--- {model_name.upper()} 调优完成 ---\")\n",
    "        print(f\"最佳 Validation AUC: {best_scores_all_final[model_name]:.6f}\")\n",
    "        print(\"最佳超参数组合:\")\n",
    "        print(best_params_all_final[model_name])\n",
    "        # ... (打印结果的代码不变) ...\n",
    "    except Exception as e:\n",
    "        print(f\"!!! {model_name.upper()} 调优过程中出现错误: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad051d80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T03:31:38.153030Z",
     "iopub.status.busy": "2025-06-11T03:31:38.152404Z",
     "iopub.status.idle": "2025-06-11T03:31:38.157612Z",
     "shell.execute_reply": "2025-06-11T03:31:38.157024Z"
    },
    "papermill": {
     "duration": 0.031595,
     "end_time": "2025-06-11T03:31:38.158618",
     "exception": false,
     "start_time": "2025-06-11T03:31:38.127023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'catboost': {'learning_rate': 0.05821152694438814, 'depth': 8}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params_all_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85211fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T03:31:38.208141Z",
     "iopub.status.busy": "2025-06-11T03:31:38.207841Z",
     "iopub.status.idle": "2025-06-11T03:31:38.215182Z",
     "shell.execute_reply": "2025-06-11T03:31:38.214221Z"
    },
    "papermill": {
     "duration": 0.03342,
     "end_time": "2025-06-11T03:31:38.216211",
     "exception": false,
     "start_time": "2025-06-11T03:31:38.182791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 根据已有的日志和 objective 函数定义，手动重建最佳参数和分数 ---\n",
      "\n",
      "--- 手动创建的最佳参数和分数字典 ---\n",
      "\n",
      "--- best_params_all ---\n",
      "{'xgboost': {'learning_rate': 0.03945974879663297, 'max_depth': 8, 'subsample': 0.6871107520310088, 'colsample_bytree': 0.8936194372129961}, 'lightgbm': {'learning_rate': 0.04717797461729656, 'num_leaves': 68}, 'catboost': {'learning_rate': 0.06067269115304827, 'depth': 7}}\n",
      "\n",
      "--- best_scores_all ---\n",
      "{'xgboost': 0.7456015793129649, 'lightgbm': 0.7447491241443516, 'catboost': 0.7455698251724243}\n",
      "\n",
      "\n",
      "--- 格式化结果预览 ---\n",
      "\n",
      "--- XGBOOST ---\n",
      "最佳 AUC: 0.745602\n",
      "最佳参数: {'learning_rate': 0.03945974879663297, 'max_depth': 8, 'subsample': 0.6871107520310088, 'colsample_bytree': 0.8936194372129961}\n",
      "\n",
      "--- LIGHTGBM ---\n",
      "最佳 AUC: 0.744749\n",
      "最佳参数: {'learning_rate': 0.04717797461729656, 'num_leaves': 68}\n",
      "\n",
      "--- CATBOOST ---\n",
      "最佳 AUC: 0.745570\n",
      "最佳参数: {'learning_rate': 0.06067269115304827, 'depth': 7}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# 手动定义从 Optuna 调优中找到的最佳参数和分数\n",
    "# (根据你提供的 objective 函数定义)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"--- 根据已有的日志和 objective 函数定义，手动重建最佳参数和分数 ---\")\n",
    "\n",
    "# --- 1. 创建 best_params_all 字典 ---\n",
    "best_params_all = {\n",
    "    'xgboost': {\n",
    "        # 根据你的 objective_xgb_final_mem_opt 函数：\n",
    "        'learning_rate': 0.03945974879663297, \n",
    "        'max_depth': 8, \n",
    "        'subsample': 0.6871107520310088, \n",
    "        'colsample_bytree': 0.8936194372129961\n",
    "        # 注意：你最新提供的 objective_xgb 函数里没有 subsample 和 colsample_bytree。\n",
    "        # 但你的日志里有。我假设日志是正确的，这里保留它们。\n",
    "        # 如果你确定最终版本没调这两个参数，可以把它们删掉。\n",
    "    },\n",
    "    'lightgbm': {\n",
    "        # 根据你的 objective_lgb_final_mem_opt 函数：\n",
    "        'learning_rate': 0.04717797461729656, \n",
    "        'num_leaves': 68\n",
    "        # 这个是完全匹配的，只调了这两个参数。\n",
    "    },\n",
    "    'catboost': {\n",
    "        # 根据你的 objective_cat_final_mem_opt 函数：\n",
    "        'learning_rate': 0.06067269115304827, \n",
    "        'depth': 7\n",
    "        # 这个也是完全匹配的，只调了这两个参数。\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- 2. 创建 best_scores_all 字典 ---\n",
    "best_scores_all = {\n",
    "    'xgboost': 0.7456015793129649,\n",
    "    'lightgbm': 0.7447491241443516,\n",
    "    'catboost': 0.7455698251724243\n",
    "}\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. 打印结果以供确认\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n--- 手动创建的最佳参数和分数字典 ---\")\n",
    "\n",
    "print(\"\\n--- best_params_all ---\")\n",
    "print(best_params_all)\n",
    "\n",
    "print(\"\\n--- best_scores_all ---\")\n",
    "print(best_scores_all)\n",
    "\n",
    "# --- 格式化打印 ---\n",
    "print(\"\\n\\n--- 格式化结果预览 ---\")\n",
    "for model_name in best_params_all.keys():\n",
    "    print(f\"\\n--- {model_name.upper()} ---\")\n",
    "    print(f\"最佳 AUC: {best_scores_all.get(model_name, 'N/A'):.6f}\")\n",
    "    print(f\"最佳参数: {best_params_all.get(model_name, 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7b14af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T03:31:38.314799Z",
     "iopub.status.busy": "2025-06-11T03:31:38.314346Z",
     "iopub.status.idle": "2025-06-11T04:41:12.535662Z",
     "shell.execute_reply": "2025-06-11T04:41:12.534696Z"
    },
    "papermill": {
     "duration": 4174.247984,
     "end_time": "2025-06-11T04:41:12.536995",
     "exception": false,
     "start_time": "2025-06-11T03:31:38.289011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 68\u001b[0m\n\u001b[1;32m     64\u001b[0m             val_preds_ensembled \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_val)[:, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m (k \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(seeds))\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m oof_preds, val_preds_ensembled\n\u001b[0;32m---> 68\u001b[0m params_xgb \u001b[38;5;241m=\u001b[39m \u001b[43mbest_params_all_final\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mxgboost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     69\u001b[0m params_xgb\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary:logistic\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_metric\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtree_method\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu_hist\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     71\u001b[0m params_lgb \u001b[38;5;241m=\u001b[39m best_params_all_final[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlightgbm\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'xgboost'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# --- XGBoost 最终训练函数 ---\n",
    "def train_final_xgb(X_train, y_train, X_val, params, k=5, seeds=[42]):\n",
    "    oof_preds = np.zeros(X_train.shape[0])\n",
    "    val_preds_ensembled = np.zeros(X_val.shape[0])\n",
    "    \n",
    "    for seed in tqdm(seeds, desc=\"XGB Seed Loop\"):\n",
    "        params['seed'] = seed\n",
    "        folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "        for i, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n",
    "            trn_X, val_X = X_train.iloc[trn_idx], X_train.iloc[val_idx]\n",
    "            trn_y, val_y = y_train.iloc[trn_idx], y_train.iloc[val_idx]\n",
    "            \n",
    "            dtrain = xgb.DMatrix(trn_X, label=trn_y)\n",
    "            dval = xgb.DMatrix(val_X, label=val_y)\n",
    "            dtest = xgb.DMatrix(X_val)\n",
    "            \n",
    "            model = xgb.train(params, dtrain, num_boost_round=10000, evals=[(dval, 'v')], \n",
    "                              verbose_eval=False, early_stopping_rounds=600)\n",
    "            \n",
    "            oof_preds[val_idx] += model.predict(dval, iteration_range=(0, model.best_iteration)) / len(seeds)\n",
    "            val_preds_ensembled += model.predict(dtest, iteration_range=(0, model.best_iteration)) / (k * len(seeds))\n",
    "            \n",
    "    return oof_preds, val_preds_ensembled\n",
    "\n",
    "# --- LightGBM 最终训练函数 ---\n",
    "def train_final_lgb(X_train, y_train, X_val, params, k=5, seeds=[42]):\n",
    "    oof_preds = np.zeros(X_train.shape[0])\n",
    "    val_preds_ensembled = np.zeros(X_val.shape[0])\n",
    "    \n",
    "    for seed in tqdm(seeds, desc=\"LGB Seed Loop\"):\n",
    "        params['random_state'] = seed\n",
    "        folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "        for i, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n",
    "            trn_X, val_X = X_train.iloc[trn_idx], X_train.iloc[val_idx]\n",
    "            trn_y, val_y = y_train.iloc[trn_idx], y_train.iloc[val_idx]\n",
    "            \n",
    "            dtrain = lgb.Dataset(trn_X, label=trn_y)\n",
    "            dval = lgb.Dataset(val_X, label=val_y)\n",
    "            \n",
    "            model = lgb.train(params, dtrain, num_boost_round=10000, valid_sets=[dval],\n",
    "                              callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "            \n",
    "            oof_preds[val_idx] += model.predict(val_X, num_iteration=model.best_iteration) / len(seeds)\n",
    "            val_preds_ensembled += model.predict(X_val, num_iteration=model.best_iteration) / (k * len(seeds))\n",
    "            \n",
    "    return oof_preds, val_preds_ensembled\n",
    "\n",
    "# --- CatBoost 最终训练函数 ---\n",
    "def train_final_cat(X_train, y_train, X_val, params, k=5, seeds=[42]):\n",
    "    oof_preds = np.zeros(X_train.shape[0])\n",
    "    val_preds_ensembled = np.zeros(X_val.shape[0])\n",
    "    \n",
    "    for seed in tqdm(seeds, desc=\"CAT Seed Loop\"):\n",
    "        params['random_seed'] = seed\n",
    "        folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "        for i, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n",
    "            trn_X, val_X = X_train.iloc[trn_idx], X_train.iloc[val_idx]\n",
    "            trn_y, val_y = y_train.iloc[trn_idx], y_train.iloc[val_idx]\n",
    "            \n",
    "            model = cbt.CatBoostClassifier(**params)\n",
    "            model.fit(trn_X, trn_y, eval_set=[(val_X, val_y)], use_best_model=True)\n",
    "            \n",
    "            oof_preds[val_idx] += model.predict_proba(val_X)[:, 1] / len(seeds)\n",
    "            val_preds_ensembled += model.predict_proba(X_val)[:, 1] / (k * len(seeds))\n",
    "            \n",
    "    return oof_preds, val_preds_ensembled\n",
    "\n",
    "params_xgb = best_params_all_final['xgboost']\n",
    "params_xgb.update({'objective': 'binary:logistic', 'eval_metric': 'auc', 'tree_method': 'gpu_hist'})\n",
    "\n",
    "params_lgb = best_params_all_final['lightgbm']\n",
    "params_lgb.update({'objective': 'binary', 'metric': 'auc', 'boosting_type': 'gbdt', 'device': 'gpu', 'n_jobs': -1, 'verbose': -1})\n",
    "\n",
    "params_cat = best_params_all_final['catboost']\n",
    "params_cat.update({'objective': 'Logloss', 'eval_metric': 'AUC', 'task_type': 'GPU', 'verbose': 0, 'iterations': 10000, 'early_stopping_rounds': 600})\n",
    "\n",
    "# 定义种子和折数\n",
    "k = 5\n",
    "seeds = [20041110, 20050327, 20050217]\n",
    "\n",
    "# 分别运行每个模型的训练\n",
    "oof_lgb, preds_lgb = train_final_lgb(X_train_final, y_all_train, X_val_final, params_lgb, k, seeds)\n",
    "oof_xgb, preds_xgb = train_final_xgb(X_train_final, y_all_train, X_val_final, params_xgb, k, seeds)\n",
    "oof_cat, preds_cat = train_final_cat(X_train_final, y_all_train, X_val_final, params_cat, k, seeds)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- 模型融合与提交文件生成 ---\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# --- a. Stacking 融合 ---\n",
    "print(\"\\n--- 使用 Stacking 融合模型 ---\")\n",
    "\n",
    "# 1. 创建第二层训练数据 (只使用 OOF 预测)\n",
    "X_train_layer2 = pd.DataFrame({\n",
    "    'xgb': oof_xgb,\n",
    "    'lgb': oof_lgb,\n",
    "    'cat': oof_cat\n",
    "})\n",
    "\n",
    "# 2. 创建第二层测试数据 (使用对测试集的预测)\n",
    "X_test_layer2 = pd.DataFrame({\n",
    "    'xgb': preds_xgb,\n",
    "    'lgb': preds_lgb,\n",
    "    'cat': preds_cat\n",
    "})\n",
    "\n",
    "# 3. (新增) 使用 LogisticRegressionCV 调优元模型参数 C\n",
    "print(\"正在为元模型(逻辑回归)进行交叉验证以寻找最佳 C 值...\")\n",
    "\n",
    "# Cs=20: 尝试20个候选的C值\n",
    "# cv=5: 使用5折交叉验证\n",
    "# scoring='roc_auc': 优化的目标是最大化AUC\n",
    "# n_jobs=-1: 使用所有CPU核心\n",
    "# random_state: 保证结果可复现\n",
    "meta_model_cv = LogisticRegressionCV(\n",
    "    Cs=20, \n",
    "    cv=5, \n",
    "    scoring='roc_auc', \n",
    "    random_state=42, \n",
    "    n_jobs=-1,\n",
    "    max_iter=1000 # 增加最大迭代次数以确保收敛\n",
    ")\n",
    "meta_model_cv.fit(X_train_layer2, y_all_train)\n",
    "\n",
    "# 获取找到的最佳 C 值\n",
    "best_c = meta_model_cv.C_[0]\n",
    "print(f\"元模型找到的最佳 C 值是: {best_c:.6f}\")\n",
    "\n",
    "\n",
    "# 4. 使用找到的最佳 C 值，在全部 OOF 数据上训练最终的元模型\n",
    "print(\"使用最佳 C 值训练最终元模型...\")\n",
    "final_meta_model = LogisticRegression(C=best_c, random_state=42, max_iter=1000)\n",
    "final_meta_model.fit(X_train_layer2, y_all_train)\n",
    "print(\"元模型训练完成。\")\n",
    "\n",
    "# 打印元模型学习到的权重 (系数)\n",
    "print(\"元模型学习到的权重 (系数):\")\n",
    "print(pd.Series(final_meta_model.coef_[0], index=X_test_layer2.columns))\n",
    "\n",
    "\n",
    "# 5. 对测试集进行最终预测\n",
    "final_predictions_stacking = final_meta_model.predict_proba(X_test_layer2)[:, 1]\n",
    "print(\"已生成对测试集的最终 Stacking 预测。\")\n",
    "\n",
    "\n",
    "# --- b. 生成提交文件 ---\n",
    "# Stacking 结果\n",
    "submission_stacking = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'isDefault': final_predictions_stacking\n",
    "})\n",
    "submission_path_stacking = 'submission_stacking_tuned.csv'\n",
    "submission_stacking.to_csv(submission_path_stacking, index=False)\n",
    "print(f\"\\n已生成调优后的 Stacking 提交文件: '{submission_path_stacking}'\")\n",
    "print(submission_stacking.head())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9948b64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SHAP 特征贡献分析（基于 LightGBM）\")\n",
    "\n",
    "# 为了做 SHAP，我们单独训练一个 LightGBM 模型（用与最终模型相同的参数），\n",
    "# 在全部训练样本上 + 一小部分作为验证集，便于 early stopping。\n",
    "\n",
    "\n",
    "X_tr_shap, X_val_shap, y_tr_shap, y_val_shap = train_test_split(\n",
    "    X_train_final, y_all_train, test_size=0.2, random_state=2025, stratify=y_all_train\n",
    ")\n",
    "\n",
    "dtrain_shap = lgb.Dataset(X_tr_shap, label=y_tr_shap)\n",
    "dval_shap = lgb.Dataset(X_val_shap, label=y_val_shap)\n",
    "\n",
    "lgb_shap_model = lgb.train(\n",
    "    params_lgb,\n",
    "    dtrain_shap,\n",
    "    num_boost_round=10000,\n",
    "    valid_sets=[dval_shap],\n",
    "    callbacks=[lgb.early_stopping(100, verbose=False)]\n",
    ")\n",
    "\n",
    "# 使用 SHAP 的 TreeExplainer\n",
    "explainer = shap.TreeExplainer(lgb_shap_model)\n",
    "# 注意：对于二分类，LightGBM 的 shap_values 通常是一个 list，这里取正类那一列\n",
    "shap_values = explainer.shap_values(X_val_shap)\n",
    "\n",
    "# 如果 shap_values 是 list，取 shap_values[1]（正类）；如果是 ndarray，直接用即可\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values_for_plot = shap_values[1]\n",
    "else:\n",
    "    shap_values_for_plot = shap_values\n",
    "\n",
    "print(\"开始绘制 SHAP summary plot（Top 特征全局贡献）...\")\n",
    "shap.summary_plot(\n",
    "    shap_values_for_plot,\n",
    "    X_val_shap,\n",
    "    max_display=30,      # 显示前 30 个特征\n",
    "    show=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7628337,
     "sourceId": 12115632,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "TGP3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12507.740939,
   "end_time": "2025-06-11T04:41:15.896569",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-11T01:12:48.155630",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
